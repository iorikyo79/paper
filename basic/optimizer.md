## 딥러닝 옵티마이저들의 실증적 비교 연구

저자: Dami Choi 외 5명

### 초록

딥러닝 파이프라인에서 옵티마이저를 선택하는 건 아주 중요한 과정이야. 이 논문에서는 옵티마이저들을 비교할 때 하이퍼파라미터 튜닝 프로토콜에 얼마나 민감한지 보여줄 거야.
우리가 발견한 바로는, 하이퍼파라미터 탐색 공간이 최근 문헌들의 실험 결과 순위를 결정짓는 가장 중요한 요소일 수 있어. 실제로 하이퍼파라미터 탐색 공간을 바꾸면 이전 연구들의 결과가 뒤집힐 수 있다는 걸 보여줄 거야.
튜닝에 들이는 노력이 무한대로 커지면, 더 일반적인 옵티마이저는 자기가 근사할 수 있는 다른 옵티마이저보다 성능이 절대 나쁠 수 없어(예를 들어 Adam은 momentum보다 절대 성능이 떨어질 수 없음). 하지만 최근의 옵티마이저 비교 연구들은 이런 포함 관계가 실제로는 중요하지 않다고 가정하거나, 아니면 하이퍼파라미터를 제한해서 이런 포함 관계를 깨뜨리고 있어.
우리 실험에서는 옵티마이저 간의 포함 관계가 실제로도 중요하고, 항상 옵티마이저 비교 결과를 예측한다는 걸 발견했어. 특히 유명한 adaptive gradient 방식들이 momentum이나 gradient descent보다 성능이 떨어지는 일은 없었어.
우리는 또한 adaptive gradient 방법들의 잘 무시되던 하이퍼파라미터들을 튜닝하는 실용적인 팁들도 공유하고, 신경망 학습을 위한 옵티마이저들을 공정하게 벤치마크하는 것에 대한 우려도 제기할 거야.

각주:
1) 옵티마이저(Optimizer): 신경망의 파라미터를 업데이트하는 알고리즘. SGD, Adam 등이 대표적임
2) 하이퍼파라미터(Hyperparameter): 학습률이나 모멘텀 계수 같이 학습 전에 수동으로 설정해야 하는 값들
3) Adaptive gradient 방식: 학습률을 자동으로 조정하는 최적화 알고리즘들(Adam, RMSprop 등)
4) 포함 관계: 한 옵티마이저가 다른 옵티마이저의 특수한 경우가 될 수 있는 관계

### 1. 서론

딥러닝 개발자가 선택하는 최적화 알고리즘은 모델의 학습 속도와 최종 성능을 결정짓는 중요한 요소야. 근데 이 선택을 어떻게 해야 하는지 제대로 설명하는 이론은 아직 없어. 그래서 우리 커뮤니티는 Wilson 등(2017)의 실험 연구나 Schneider 등(2019)의 벤치마킹 같은 경험적인 연구에 의존하고 있는 거지.
실제로 새로운 옵티마이저를 소개하는 논문들은 다양한 워크로드*에서 광범위한 비교 실험 결과를 보고하는 게 표준이 됐어. 그래서 과학적 발전을 극대화하려면, 최적화 알고리즘들을 비교하는 우리의 능력을 신뢰할 수 있어야 해.
옵티마이저를 비교하는 이론적 가이드는 없지만, 유명한 1차 옵티마이저들은 자연스러운 포함 관계를 형성하고 있어. 예를 들어 ADAM과 RMSPROP은 파라미터 업데이트의 분모에 있는 ε 항을 매우 크게 하면 MOMENTUM을 근사적으로 시뮬레이션할 수 있지.
하지만 이런 관계가 실제로는 별로 중요하지 않을 수도 있어. 예를 들어, ADAM이 MOMENTUM의 성능과 비슷하거나 더 좋은 성능을 내도록 하는 하이퍼파라미터 설정이 너무 찾기 어려울 수 있거든(예: 무한대에 가까운 값이 필요하다든지).
이 논문에서는 신경망 옵티마이저들을 실증적으로 비교할 때 중요한 두 가지 서로 연관된 포인트를 보여줄 거야:

옵티마이저들 간의 포함 관계가 실제로도 중요하다는 걸 보여줄 거야. 우리 실험에서는 더 일반적인 옵티마이저가 특수한 경우보다 성능이 떨어지는 일은 없었어. 통념(Wilson 등, 2017; Balles & Hennig, 2017)과는 달리, 제대로 튜닝하면 ADAM이나 다른 adaptive gradient 방식들이 MOMENTUM이나 SGD보다 성능이 떨어지는 일은 없다는 걸 발견했어.
옵티마이저 비교가 하이퍼파라미터 튜닝 프로토콜에 얼마나 민감한지 보여줄 거야. 이전 실험 평가들과 비교해보면, 하이퍼파라미터 튜닝 프로토콜을 바꾸는 것만으로도 주어진 워크로드에서 옵티마이저 순위가 얼마나 쉽게 바뀌는지 보여줄 수 있어. 튜닝에 더 많은 노력을 들일수록 옵티마이저 순위는 포함 관계에 따라 안정화돼.


각주:
1) 워크로드: 특정 모델과 데이터셋의 조합을 의미함. 예를 들어 "ResNet으로 ImageNet 학습하기" 같은 걸 워크로드라고 함

### 2. 배경 및 관련 연구

우리 연구는 Wilson 등(2017)과 Schneider 등(2019)이 최근에 한 신경망 옵티마이저 연구에서 영감을 받았어.
Wilson 등(2017)은 adaptive gradient 방법들(예: ADAM)이 표준 gradient 방법보다 확실히 더 나쁜 해에 수렴한다는 걸 증명할 수 있는 간단한 분류 문제를 만들었어. 근데 결정적으로, 그들의 분석은 일부 adaptive gradient 방법의 분모에 있는 ε 파라미터를 무시했다는 문제가 있어.
Wilson 등은 또 실제 딥러닝 워크로드에서도 실험을 했는데, 거기서는 ADAM이 SGD보다 모든 검증 정확도가 낮게 나왔어. 하지만 그들은 학습률과 학습률 감소 스케줄만 튜닝하고 ADAM의 다른 파라미터들은 다 기본값으로 고정해뒀었지. 이런 결과에도 불구하고 Wilson 등의 연구 이후에도 adaptive gradient 방법들은 계속 인기를 끌었어.
Schneider 등(2019)은 딥러닝 옵티마이저를 위한 벤치마크 세트(DEEPOBS)를 발표하고, 자기들이 시도한 모든 워크로드에서 "이게 제일 좋다"고 할 만한 옵티마이저는 없다고 보고했어. 근데 Schneider 등도 각 옵티마이저의 학습률만 튜닝하고 다른 하이퍼파라미터는 다 기본값으로 고정해뒀었지.
4.3절에서 자세히 설명하겠지만, Wilson 등(2017)과 Schneider 등(2019)이 선택한 하이퍼파라미터 튜닝 프로토콜 때문에 그들의 결과가 실제 어떤 옵티마이저를 쓸지 결정하는데는 별로 관련이 없을 수 있어.
하이퍼파라미터 튜닝은 딥러닝 파이프라인에서 아주 중요한 단계거든(Bergstra & Bengio, 2012; Snoek 등, 2012; Sutskever 등, 2013; Smith, 2018). 그래서 옵티마이저를 연구하는 논문들은 이상적인 실무자가 하는 튜닝 방식을 최대한 가깝게 따라야만 해.
하지만 옵티마이저를 연구하는 논문들의 튜닝 프로토콜은, 특정 문제를 풀기 위해 신경망을 학습시키는 논문들의 튜닝 방식과는 많이 다른 경우가 많아.

참고:
1) 하이퍼파라미터 튜닝: 모델의 학습률, 배치 크기 등을 조정하는 과정
2) 검증 정확도: 학습에 사용되지 않은 검증 데이터셋에서의 성능
3) 기본값(default value): 라이브러리에서 기본으로 설정된 값

최근에 최적화 알고리즘을 연구하거나 소개하는 논문들은 실험을 단순화하려고 ADAM과 RMSPROP의 ε 하이퍼파라미터는 튜닝 안 하고 비교하는 경향이 있어. 보통 ADAM은 ε=10⁻⁸, RMSPROP은 ε=10⁻¹⁰ 같은 일반적인 기본값을 그대로 쓰는 게 표준이 됐지(여러 연구 인용).
심지어 어떤 논문들은 사용한 ε 값조차 보고 안 하기도 해(여러 연구 인용).
예외도 있긴 해. Zaheer 등(2018)이랑 Liu 등(2019)은 표준 기본값보다 훨~~씬 큰 ε 값들도 고려했어. 근데 이 논문들도 ADAM을 튜닝할 때 ε 값을 최대 두 개만 테스트했다는 한계가 있어. De 등(2018)이 우리가 찾은 유일하게 넓은 범위의 ε 값을 고려한 연구였어. Zaheer 등(2018)과 De 등(2018) 모두 기본값이 아닌 ε 값들이 기본값보다 더 좋은 성능을 냈다는 걸 발견했어.
응용 분야에서도 기본 ε 값을 쓰는 게 엄청 흔하긴 한데, 몇몇 중요한 논문들은 ε을 튜닝해서 기본값과 8배나 차이 나는 값을 선택하기도 했어:

Szegedy 등(2016)은 RMSPROP에서 ε=1 사용
1) Liu 등(2019)은 결과가 ε에 민감하다고 보고하고 ADAM에서 ε=10⁻⁶ 사용
2) Tan 등(2019)과 Tan & Le(2019)는 RMSPROP에서 ε=10⁻³ 사용. 후자는 ImageNet에서 최고의 top-1 정확도를 달성
3) 강화학습에서는 Hessel 등(2017)이 ε=1.5×10⁻⁴ 사용

ADAM의 ε은 원래 0으로 나누는 걸 막으려고 도입된 거였는데¹, 최적의 선택이 문제마다 다를 것 같은 여러 해석이 가능해:

1) ADAM을 자연 기울기 하강법²의 실험적/대각선적 근사로 해석하면(Kingma & Ba, 2015), ε은 Fisher 행렬의 조건을 개선하는 다목적 댐핑 항으로 볼 수 있어. 이건 Becker & Le Cun(1988)이 고려한 근사적 2차 방법과 비슷해.
2) ε을 신뢰 영역 반경을 설정하는 것으로 볼 수도 있고(Martens & Grosse, 2015; Adolphs 등, 2019), momentum과 대각 자연 기울기 하강법 사이의 보간을 제어하는 걸로도 볼 수 있어. v_t가 업데이트 방향에 미치는 영향을 감소시키거나 증가시킴으로써 말이야.

어떤 해석을 하든 ε의 최적값은 문제에 따라 다를 거고, 아마 튜닝하면 도움될 거야.

각주:
¹ 텐서플로우는 현재 ε을 "수치 안정성을 위한 작은 상수"라고 설명하고 있음
² 자연 기울기 하강법(Natural Gradient Descent): 파라미터 공간의 곡률을 고려하는 최적화 방법

### 3. 옵티마이저가 뭐야?

최적화 알고리즘은 보통 업데이트 규칙으로 정의되는데, 이건 하이퍼파라미터로 제어돼. 미분 가능한 손실 함수 ℓ: Rᵈ → R을 생각해보자. 여기서 벡터의 첫 번째 편미분은 ∇ℓ(θ)로 표시해(더 일반적으로는, ∇ℓ(θ)가 진짜 기울기의 확률적 추정치일 수도 있어).
우리 맥락에서 ℓ은 신경망이 전체 데이터셋에 대해 계산한 손실 함수고, θ ∈ Rᵈ는 모델 파라미터 벡터를 나타내. 최적화 문제는 (적어도 국소적으로) ℓ을 최소화하는 지점을 찾는 거야.
이 문제를 푸는 1차 반복 방법들(Nesterov, 2018)은 ℓ과 ∇ℓ을 쿼리해서 θ*라는 국소 최소값으로 수렴하는 반복값 θₜ의 수열을 만들어내. 이 수열 θₜ는 업데이트 규칙 M에 의해 만들어지는데, M은 다음 반복값 θₜ₊₁을 이전 반복값들의 히스토리 Hₜ와 하이퍼파라미터 설정 φ: N → Rⁿ을 이용해서 결정해:
Hₜ = {θₛ, ∇ℓ(θₛ), ℓ(θₛ)}ₜₛ₌₀
초기 파라미터 값 θ₀ ∈ Rᵈ가 주어지면, 업데이트 규칙 M을 가진 옵티마이저가 방문하는 점들의 수열은 다음과 같이 주어져:
θₜ₊₁ = M(Hₜ, φₜ)
확률적 경사 하강법(SGD; Robbins & Monro, 1951)은 신경망 학습에 사용되는 가장 간단한 방법 중 하나야. SGD는 θ₀ ∈ Rᵈ로 초기화되고, 하이퍼파라미터는 학습률 스케줄 η: N → (0, ∞)이야. SGD 업데이트 규칙은 SGD(Hₜ, ηₜ) = θₜ - ηₜ∇ℓ(θₜ)로 주어져.
Polyak(1964)의 MOMENTUM 방법은 기울기 방향과 이전 파라미터 업데이트의 선형 조합을 사용해서 SGD를 일반화해. 하이퍼파라미터는 학습률 스케줄 η: N → (0, ∞)와 모멘텀 파라미터 γ ∈ [0, ∞)야:
MOMENTUM(Hₜ, ηₜ, γ) = θₜ - ηₜ∇ℓ(θₜ) + γ(θₜ - θₜ₋₁)

각주:

1) 확률적 경사 하강법(SGD): 전체 데이터 대신 미니배치를 사용해 파라미터를 업데이트하는 방법
2) 모멘텀(Momentum): 이전 업데이트 방향을 고려해 현재 업데이트를 조절하는 방법
3) 하이퍼파라미터: 알고리즘의 동작을 제어하는 설정값들

딥러닝에서는 이런 1차 방법들이 엄청나게 많이 나왔는데, 다 이런 표준적인 1차 방식의 틀에 들어가. Table 1에 우리가 이 논문에서 고려한 1차 업데이트 규칙들을 정리해놨어.
옵티마이저들의 차이는 업데이트 규칙 M과 하이퍼파라미터 φ의 선택으로 완전히 설명돼. 신경망 손실 함수에서 옵티마이저 하이퍼파라미터들의 역할이 잘 이해되지 않았기 때문에, 대부분의 실무자들은 검증 세트에서의 성능을 최대화하도록 일부 하이퍼파라미터만 튜닝하고 나머지는 고정된 기본값으로 둬.
어떤 하이퍼파라미터를 튜닝할지 선택하는 건 실질적인 업데이트 규칙 집합을 결정하고, 이게 실무자 관점에서 중요한 거야. 그래서 C++의 함수 오버로딩처럼, 우리는 옵티마이저를 "시그니처"로 정의해 - 업데이트 규칙 이름과 자유 하이퍼파라미터 인자를 함께 써서.
예를 들어, 이 정의에서 MOMENTUM(·, ηₜ, γ)는 MOMENTUM(·, ηₜ, 0.9)와 다른 옵티마이저야. 후자는 자유 하이퍼파라미터가 하나뿐이지만 전자는 두 개거든. 기본 ε을 쓰는 ADAM은 ε을 튜닝하는 ADAM과는 "다른" 거야.

#### 3.1 1차 방법들의 분류

이 섹션의 기본 관찰은 일부 옵티마이저들이 다른 것들을 근사적으로 시뮬레이션할 수 있다는 거야(즉, 옵티마이저 A가 옵티마이저 B의 하이퍼파라미터 어떤 설정에 대해서도 그 궤적을 근사적으로 시뮬레이션할 수 있을 수도 있어).
이건 중요한 지식이야. 왜냐하면 하이퍼파라미터 튜닝 프로토콜이 최적에 가까워질수록, 더 표현력이 좋은 옵티마이저는 자신의 특수화된 버전보다 절대 성능이 떨어질 수 없거든. 한 옵티마이저가 다른 걸 근사하는 개념을 포착하기 위해, 다음과 같은 옵티마이저들 간의 포함 관계를 정의해.

각주:

1) 시그니처(signature): 함수나 메서드의 이름과 파라미터 목록
2) 특수화(specialization): 더 일반적인 형태에서 특수한 경우로 제한된 형태

정의 1 (포함 관계)

M과 N이 1차 최적화 방법에서 사용되는 업데이트 규칙이라고 하자. 모든 φ: N → Rⁿ에 대해 아래 조건을 만족하는 수열 ψᵢ: N → Rᵐ이 존재하면, M은 N의 부분집합 또는 특수화라고 해:
모든 t ∈ [0, ∞)와 히스토리 Hₜ에 대해,
limᵢ→∞ N(Hₜ, ψᵢₜ) = M(Hₜ, φₜ)
이걸 M ⊆ N으로 표기하고, M ⊆ N이고 N ⊆ M이면 M = N이야.
당연히 SGD ⊆ MOMENTUM이야. SGD(Hₜ, ηₜ) = MOMENTUM(Hₜ, ηₜ, 0)이니까. 많은 유명한 옵티마이저들이 이런 분류 체계에 자연스럽게 들어가. 특히 우리는 모멘텀이 있는 RMSPROP(Tieleman & Hinton, 2012), ADAM(Kingma & Ba, 2015), NADAM(Dozat, 2016)을 살펴봤는데(Table 1 참고), 부록에서 다음과 같은 포함 관계를 보여줄 거야:
SGD ⊆ MOMENTUM ⊆ RMSPROP
SGD ⊆ MOMENTUM ⊆ ADAM
SGD ⊆ NESTEROV ⊆ NADAM
참고로, 이런 포함 관계 중 일부는 하이퍼파라미터 스케줄의 유연성(ψᵢ가 t에 의존하는 것)을 활용해. 특히 MOMENTUM을 ADAM으로 근사하려면 ADAM의 편향 보정을 고려하는 학습률 스케줄을 선택해야 해.
만약 두 옵티마이저 사이에 포함 관계가 있다면, 관심 있는 어떤 메트릭에 대해서도 더 일반적인 옵티마이저가 절대로 더 나쁠 수가 없어. 물론 이건 그 메트릭을 최적화하도록 하이퍼파라미터들이 충분히 잘 튜닝됐다는 가정 하에서야.
최적으로 튜닝된 MOMENTUM은 최적으로 튜닝된 SGD보다 성능이 떨어질 수 없어. MOMENTUM에서 γ=0으로 설정하면 SGD가 되니까.

각주:

1) 편향 보정(bias correction): ADAM에서 초기 단계의 모멘텀 추정치를 보정하는 기법
2) 메트릭(metric): 성능을 측정하는 지표. 예: 정확도, 손실값 등

<img width="844" alt="image" src="https://github.com/user-attachments/assets/546f263c-baa8-4073-9622-01e559c02704">

하지만 하이퍼파라미터가 더 많은 옵티마이저는 튜닝하는데 더 비용이 많이 들 수 있어. 그래서 더 일반적인 옵티마이저를 쓰거나 만들려면 이론적이나 실험적인 근거가 있어야 해.
예를 들어, MOMENTUM은 두 번 미분 가능하고 매끄럽고 강볼록인 함수들에서 SGD보다 더 나은 국소 수렴 속도를 보여(Polyak, 1964). NESTEROV는 매끄럽고 강볼록인 함수들의 클래스 내에서 전역적으로 최적의 수렴 속도를 가져(Nesterov, 1983; 2018).
얼핏 보면 옵티마이저 포함 관계의 분류가 많은 옵티마이저 비교 문제를 해결하는 것 같아 보여. 하지만 딥러닝 실무자 입장에서는 포함 관계 계층이 실제로 의미가 있을 거라는 보장이 없어.
예를 들어, ADAM이 MOMENTUM과 같거나 더 나은 성능을 내게 하는 하이퍼파라미터들이 쉽게 찾을 수 없을 수도 있어. 그 값들이 아주 큰 값의 극한에서만 존재하거나, 엄청난 컴퓨팅 자원이 있는 실무자들만 찾을 수 있을 정도로 찾기 어려울 수 있지.
실제로 실증 연구들과 통념은 포함 관계 계층이 많은 실용적인 워크로드에서 옵티마이저 성능을 예측하지 못한다고 봐(Wilson 등, 2017; Balles & Hennig, 2017; Schneider 등, 2019).
이 실험 연구들이 너무 제한적이거나, 아니면 이 섹션의 분류법이 실용적 관심사가 제한적이고 실제 워크로드에서 어떤 옵티마이저를 써야 할지에 대한 지침을 제공하지 못하는 거야.
다음 섹션에서는 이 질문에 실험적으로 답하려고 시도할 거고, 이 포함 관계들이 실제로도 의미가 있다는 걸 보여줄 거야.

각주:

1) 강볼록(strongly convex): 최적화에서 함수가 유일한 최소값을 가지는 강한 조건
2) 수렴 속도(convergence rate): 알고리즘이 최적해에 얼마나 빨리 접근하는지를 나타내는 척도
3) 매끄러움(smoothness): 함수가 연속적이고 미분 가능한 정도

### 4. 실험

실험 접근 방식

옵티마이저들의 실증적 비교는 꼼꼼한 실무자에게 도움이 되는 걸 목표로 해야 해. 그래서 우리는 프로토콜을 짤 때, 각 옵티마이저의 모든 하이퍼파라미터를 다 조정할 수 있는 실무자를 모델로 삼았어(예: ADAM의 경우 αₜ, β₁, β₂, ε 전부). 이건 하이퍼파라미터의 일부를 기본값으로 고정한 이전 연구들(Wilson 등, 2017; Schneider 등, 2019)과는 대조적이야.
이런 하이퍼파라미터 값들을 선택하는 표준 방법은 없지만, 대부분의 실무자들은 최소한 일부 하이퍼파라미터는 검증 세트에서 성능을 최대화하도록 여러 번 시도해서 튜닝해. 우리 실험에서는 워크로드마다 수십에서 수백 번의 개별 시도를 했어. 우리가 고려한 워크로드의 다양성을 생각하면, 이 시도 횟수는 꽤 넓은 범위의 컴퓨팅 예산을 커버해.
각 옵티마이저의 하이퍼파라미터 탐색 공간을 선택하는 건 옵티마이저들을 실증적으로 비교할 때 핵심적인 방법론적 선택이야. 이전 연구들은 모든 옵티마이저를 공평하게 다루려고 "같은" 탐색 공간을 사용하려고 했어(예: Wilson 등, 2017; Schneider 등, 2019).
하지만 이러려면 비슷한 이름의 하이퍼파라미터들이 옵티마이저들 사이에서 비슷한 값을 가져야 한다고 가정해야 하는데, 이건 일반적으로 사실이 아니야. 예를 들어:
MOMENTUM과 NESTEROV는 둘 다 비슷해 보이는 모멘텀과 학습률 하이퍼파라미터를 가지고 있지만, NESTEROV는 더 큰 모멘텀 값을 허용해(Sutskever 등, 2013).
덜 관련된 옵티마이저들 사이에서는 상황이 더 안 좋아져: 비슷한 이름의 하이퍼파라미터들이 완전히 다른 단위를 가질 수 있어서 탐색 볼륨을 동등하게 만드는 게 불가능할 수도 있어.

각주:

1) 워크로드: 특정 모델과 데이터셋의 조합
2) 검증 세트: 학습에 사용되지 않는 별도의 데이터셋으로, 모델의 성능을 평가하는 데 사용
3) 탐색 공간: 하이퍼파라미터가 가질 수 있는 가능한 값들의 범위
4) 탐색 볼륨: 하이퍼파라미터 공간에서 실제로 탐색하는 영역의 크기

<img width="844" alt="image" src="https://github.com/user-attachments/assets/10fa9565-1c46-492c-8095-4bc563b5e084">

비록 나름의 도전과제가 있긴 하지만, 실무자가 다른 옵티마이저별로 독립적으로 하이퍼파라미터를 튜닝하도록 하는 게, 즉 옵티마이저별로 특화된 탐색 공간을 사용하는 게 가장 유용한 정보를 제공해.
우리 실험에서는 각 옵티마이저의 탐색 공간을 상대적으로 큰 초기 탐색 공간에서 실험을 돌려서 선택했어. 보통은 최종 탐색 공간을 선택하기 위해 옵티마이저별로 한 세트의 초기 실험만 했지.
하지만 몇몇 경우에는 탐색 공간 중 하나를 잘못 선택해서, 최종 탐색 공간을 선택하기 위한 실험을 한 번 더 해야 했어. 투명성을 위해 이런 초기 탐색 공간들을 부록 D에 다 포함시켜놨어.
각 탐색 공간을 선택하는 데 필요한 노력은 쉽게 정량화할 수 없어. 우리의 초기 추측들은 불가피하게 특정 모델과 옵티마이저들에 대한 이전 경험의 영향을 받았거든. 이건 문헌의 모든 탐색 공간에도 해당되는 얘기야: 튜닝 방식은 보통 여러 실험과 워크로드를 거치면서 개선되고, 이는 우리 커뮤니티 전체의 경험을 대표하는 거지.
표준 관행을 따라서, 우리는 Table 1의 η₀, 1-γ, 1-β₁, 1-β₂ 하이퍼파라미터들을 로그 스케일로 튜닝했어. 즉 그 값들의 로그를 검색한 거야.
추가로, RMSPROP에서는 (ε, α₀/√ε)의 로그를, ADAM과 NADAM에서는 (ε, α₀/ε)의 로그를 검색함으로써 ε을 초기 학습률 α₀와 분리했어. ε이 학습률과 연결되어 있다는 사실(보통 더 큰 ε값은 더 큰 학습률이 필요함)은 Savarese 등(2019)이 ADAM에 대해 보여줬어.

각주:

1) 로그 스케일(log scale): 값을 로그 변환해서 넓은 범위의 값을 효율적으로 탐색하는 방법
2) α₀: 초기 학습률
3) ε: 수치 안정성을 위한 작은 상수

이런 탐색 공간 변환들은 단순히 우리의 하이퍼파라미터 탐색을 더 효율적으로 만들기 위한 거야. 원칙적으로는 더 많은 시도 횟수를 쓰고 모든 하이퍼파라미터를 선형 스케일로 무식하게 찾아도 같은 결과가 나왔을 거야.
우리는 모든 실험에서 모든 옵티마이저의 최적 하이퍼파라미터 값들이 탐색 공간의 경계에서 떨어져 있는지 체크함으로써 우리의 최종 탐색 공간을 검증했어(부록 E의 Figure 5 참고). 우리의 최종 오류율이 이전에 발표된 결과들과 잘 비교된다는 점 - 특히 ResNet-50을 특정 설정으로 ImageNet에서 돌렸을 때 SOTA¹를 달성했다는 점(4.2절 참고) - 은 우리의 방법론이 전문가의 튜닝 절차와 충분히 경쟁력 있다는 걸 뒷받침해.

<img width="902" alt="image" src="https://github.com/user-attachments/assets/30d71966-d67e-4def-997a-88d276bd01c1">

위 그래프는 각 워크로드별로 6개의 옵티마이저(SGD, Momentum, Nesterov, RMSProp, Adam, NAdam)의 성능을 보여줘
위쪽 그래프는 검증 오류를, 아래쪽 그래프는 테스트 오류를 보여줌
오차 막대는 성능의 불확실성 범위를 나타냄
더 일반적인 옵티마이저가 항상 자신의 특수한 경우보다 같거나 더 좋은 성능을 보이는 걸 확인할 수 있어

이 그림은 논문의 핵심 주장 - 옵티마이저 포함 관계가 실제로도 의미가 있다 - 을 시각적으로 뒷받침하는 중요한 증거임.

<img width="887" alt="image" src="https://github.com/user-attachments/assets/c5011ac5-88e4-46c0-9586-004fe10f3941">

"옵티마이저들의 상대적인 학습 속도도 포함 관계와 일관성을 보여줘. 우리는 (이상적인) 학습 속도를 목표 검증 오류¹에 도달하는 데 필요한 학습 스텝 수로 측정했어(목표 오류값들은 Table 2를 참고)."
[그래프 설명]

이 그래프는 각 옵티마이저가 목표 성능에 도달하는 데 얼마나 많은 스텝이 필요한지를 보여줌
더 적은 스텝 수는 더 빠른 학습 속도를 의미해
여기서도 더 일반적인 옵티마이저들이 자신의 특수한 경우보다 항상 같거나 더 적은 스텝을 필요로 함
특히 Adam과 NAdam이 대부분의 경우 가장 빠른 학습 속도를 보여줌


각주:
¹ 목표 검증 오류: 연구자들이 "충분히 좋다"고 미리 정해놓은 성능 수준. 예를 들어 "90% 정확도" 같은 목표치
이 그래프는 더 일반적인 옵티마이저들이 단순히 최종 성능만 좋은 게 아니라, 그 성능에 도달하는 속도도 더 빠르다는 걸 보여주는 중요한 증거야.

<img width="902" alt="image" src="https://github.com/user-attachments/assets/1b156c21-e03c-482f-b16e-1ec8837fe6f7">

"더 많은 하이퍼파라미터를 튜닝하면 Wilson 등(2017)이 관찰했던 옵티마이저들 간의 테스트 오류 차이가 사라져.
(왼쪽) 옵티마이저 하이퍼파라미터의 일부와 초기 학습률만 튜닝해도 모든 옵티마이저 간의 성능을 동등하게 만들기에 충분했어¹.
(오른쪽) 우리 셋업에서 학습률 스케줄을 포함한 더 광범위한 하이퍼파라미터 튜닝은 모든 옵티마이저의 결과를 개선했고, 여전히 옵티마이저들 간의 성능 차이를 만들어내지 않았어²."
[그래프 설명]

그래프는 서로 다른 튜닝 전략에 따른 테스트 오류를 보여줌
왼쪽: Wilson 등의 원본 코드를 사용한 결과
오른쪽: 우리의 더 확장된 튜닝 방식 결과
막대의 높이는 테스트 오류율을 나타냄
오차 막대는 불확실성 범위를 보여줌


각주:
¹ 이는 Wilson 등의 연구가 하이퍼파라미터 튜닝의 중요성을 과소평가했다는 걸 보여줌
² 이는 "Adam이 SGD보다 성능이 나쁘다"는 Wilson 등의 주장이 제한된 실험 설계 때문이었음을 증명함
이 그래프는 논문의 중요한 반증 사례를 보여주는데, 이전 연구의 결론이 불충분한 하이퍼파라미터 튜닝 때문이었다는 걸 명확하게 보여주고 있어.


#### 4.1 워크로드와 실험 세부사항 개요

우리는 다양한 이미지 분류와 언어 모델링 태스크에서 옵티마이저들의 상대적 성능을 조사했어.
이미지 분류에서는:

- Fashion MNIST에서 간단한 CNN
- CIFAR-10에서 ResNet-32
- CIFAR-100에서 CNN
- CIFAR-10에서 VGG-16
- ImageNet에서 ResNet-50

언어 모델링에서는:

- 톨스토이의 '전쟁과 평화'에서 2층 LSTM 모델
- LM1B에서 Transformer

모든 워크로드에 대해 Shallue 등(2019)과 같은 방식으로 파라미터화된 선형 학습률 감소 스케줄을 사용했어. 각 워크로드마다 옵티마이저와 무관하게 고정된 배치 크기와 고정된 학습 스텝 예산을 썼어. Table 2가 이 워크로드들을 요약하고 있고, 부록 B에서 전체 세부사항을 제공해.

각주:
¹ SOTA(State-Of-The-Art): 현재 달성 가능한 최고 수준의 성능
² 선형 학습률 감소(linear learning rate decay): 학습률을 시간에 따라 선형적으로 줄이는 방식
³ 배치 크기(batch size): 한 번의 업데이트에 사용되는 데이터 샘플의 수
⁴ 파라미터화(parameterization): 시스템의 특성을 파라미터로 표현하는 방식

탐색 공간이 주어졌을 때, 우리의 튜닝 프로토콜은 고정된 횟수의 시도(워크로드에 따라 10, 50, 100회)로 최선의 결과를 얻으려는 실무자를 모델링하려고 했어.
가능한 시도²는 유한한 학습 손실을 달성하는 모든 시도를 말해. 우리는 각 워크로드에 맞는 탐색 공간에서 준무작위 균일 탐색(Bousquet 등, 2017)을 사용했고, 가능한 시도의 고정된 수를 얻을 때까지 탐색을 계속했어.
그런 시도들로부터 우리는 두 가지 통계를 고려했어:

최선의 결과를 특성화하기 위해서, 우리가 관심 있는 어떤 메트릭(예: 테스트 정확도)이 다른 메트릭(예: 검증 정확도)을 최적화하는 시도에 해당하는 걸 봤어.
학습 속도를 특성화하기 위해서, 탐색에서 적어도 하나의 시도가 그 타겟에 도달했다는 조건 하에 고정된 검증 타겟에 도달하는 데 필요한 스텝 수를 봤어. 각 워크로드의 타겟은 초기 실험과 문헌에서 알려진 값들을 기반으로 선택했어(Table 2 참고).

평균과 불확실성은 부록 C에 설명된 부트스트랩³ 절차를 사용해서 추정했어.

#### 4.2 포함 관계가 실제로도 중요해

Figure 1은 검증 오류를 최소화하도록 하이퍼파라미터를 튜닝한 후 6개 옵티마이저의 최종 예측 성능을 4개의 다른 워크로드에서 보여줘.
최종 검증 오류든 테스트 오류든 비교하면, 포함 관계가 모든 경우에서 성립해 - 즉, 더 일반적인 옵티마이저가 오차 막대 내에서 자신의 특수화보다 성능이 떨어지는 일은 절대 없었어. 학습 오류에서도 비슷한 결과가 나와(부록 E의 Figure 9 참고).

각주:
¹ 준무작위 균일 탐색: 완전한 무작위보다 더 균일하게 공간을 탐색하는 방법
² 가능한 시도(feasible trial): 발산하지 않고 정상적으로 학습이 완료된 실험
³ 부트스트랩(bootstrap): 데이터를 재샘플링해서 통계적 추정을 하는 방법

학습 속도도 중요한 고려사항이야. Figure 2는 포함 관계가 목표 검증 오류에 도달하는 데 필요한 스텝 수를 비교할 때도 오차 범위 내에서 성립한다는 걸 보여줘. 게다가 이렇게 옵티마이저 포함 관계의 관련성을 확인하는 우리 결과는 우리가 선택한 정확한 스텝 예산이나 오류 목표에 의존하지 않아(부록 E의 Figure 10 참고). 물론 이 값들을 크게 바꾸려면 새로운 실험이 필요하겠지만.
물론 더 일반적인 옵티마이저가 자신의 특수화보다 안 나쁘다고 해서 옵티마이저 선택이 모든 워크로드에서 큰 차이를 만드는 건 아니야. Figure 1과 2의 일부 워크로드에서는 모든 옵티마이저가 비슷한 성능을 보이는 반면, 다른 워크로드들은 확실한 순위가 있거나 극적인 차이를 보여.
예를 들어:

CIFAR-10의 ResNet-32에서는 옵티마이저 선택이 별로 중요하지 않아 보여. 모든 옵티마이저가 비슷한 예측 성능과 학습 속도를 보여줬거든.
반면에 LM1B의 Transformer는 예측 성능과 학습 속도 면에서 확실한 순위를 보여줘. 이 워크로드에서 ADAM은 목표 오류에 도달하는데 MOMENTUM보다 60%의 스텝만 필요했고, SGD와 같은 최종 결과를 얻는 데는 25%의 스텝만 필요했어(부록의 Figure 10 참고).

이런 차이는 실무자에게 분명히 중요할 만큼 크고, 일부 워크로드에서는 올바른 옵티마이저를 선택하는 게 실제로 중요하다는 걸 강조해줘.

각주:

1) 오차 범위(error bars): 측정의 불확실성을 나타내는 범위
2) 워크로드: 특정 모델과 데이터셋의 조합
3) 스텝: 파라미터 업데이트 한 번을 의미함

우리가 고려한 가장 일반적인 옵티마이저는 RMSPROP, ADAM, NADAM이었어. 이들은 서로를 특수한 경우로 포함하지 않고, 포함 관계로는 상대적 성능을 예측할 수 없지. 우리가 살펴본 워크로드들에서 이 옵티마이저들 중 명확한 승자는 없었지만, ADAM과 NADAM이 일반적으로 RMSPROP보다는 좀 더 나은 것 같았어.
이 옵티마이저들 모두에서, 가끔은 ε 파라미터를 기본값보다 몇 차수나 크게 설정해야 좋은 결과를 얻을 수 있었어. 특히 ImageNet의 ResNet-50에서 ε=9475인 NADAM으로 77.1%의 검증 정확도를 달성했는데, 이는 MOMENTUM을 사용한 Goyal 등(2017)의 76.5%를 뛰어넘는 거야.
이 4개의 워크로드에서만 보더라도, ε 파라미터의 최적값 범위가 10차수나 됐어. 미친거 아니냐?¹

#### 4.3 이전 연구들과의 차이점 해결하기

Wilson 등(2017)과 Schneider 등(2019)의 결론과 우리 결론의 차이가 하이퍼파라미터 튜닝 프로토콜의 차이 때문이라는 걸 확인하기 위해, 우리는 그들의 결과 중 대표적인 일부를 재현하고 나서 하이퍼파라미터 탐색 공간을 확장하는 것만으로도 옵티마이저 순위를 뒤집거나 적어도 무너뜨릴 수 있다는 걸 보여줬어.
Figure 3의 왼쪽 부분은 Wilson 등(2017)이 공개한 코드를 사용한 CIFAR-10의 VGG에 대한 우리 실험을 보여줘. 우리가 그들의 프로토콜을 따르고 초기 학습률만 격자 탐색²했을 때는 그들의 원래 결과 - RMSPROP과 ADAM이 더 나쁜 테스트 오류를 보인다는 걸 재현했어.
하지만 모멘텀 파라미터와 ε을 무작위 탐색으로 튜닝했을 때는, 4개의 옵티마이저 모두가 거의 동일한 테스트 오류율에 도달했어.

각주:
¹ 이는 이전 연구들이 ε을 고정값으로 두고 실험한 게 얼마나 제한적이었는지 보여줌
² 격자 탐색(grid search): 가능한 모든 하이퍼파라미터 조합을 규칙적인 간격으로 시도하는 방법

우리의 학습률 스케줄 탐색 공간에서는, 단순히 학습률 스케줄만 튜닝해도 모든 옵티마이저가 오차 범위 내에서 같은 테스트 오류에 도달할 수 있었어. 우리 셋업에서 최적화 하이퍼파라미터와 가중치 감쇠³까지 추가로 튜닝했을 때도 모든 옵티마이저가 비슷한 결과를 보였고, 포함 관계가 실제로 위반될 수 있다는 어떤 증거도 찾지 못했어.
Figure 4는 Schneider 등(2019)의 실험과 매칭되는 CIFAR-100의 CNN과 '전쟁과 평화'로 학습한 LSTM 언어 모델에 대한 우리의 다른 튜닝 프로토콜 결과를 보여줘.
Schneider 등(2019)이 보고한 것처럼, 만약 학습률 감소 스케줄이나 다른 옵티마이저 하이퍼파라미터를 튜닝하지 않고 학습률만 튜닝하면:

CNN에서는 ADAM이 MOMENTUM보다 성능이 더 안 좋았고
'전쟁과 평화' 데이터셋에서는 SGD가 ADAM과 MOMENTUM보다 약간 더 좋은 성능을 보였어 (근데 Schneider 등은 SGD가 더 큰 이점이 있다고 발견했지)

하지만 일단 모든 옵티마이저 하이퍼파라미터를 전부 튜닝하면, ADAM이 MOMENTUM보다 더 잘하고 MOMENTUM이 SGD보다 더 잘해. 이건 포함 관계가 예측한 대로야.
우리는 Schneider 등(2019)과 Wilson 등(2017) 모두 하이퍼파라미터를 충분히 튜닝하지 않았기 때문에 처음 보기에 포함 관계와 모순되는 것 같은 순위를 관찰했다고 결론 내렸어.
만약 우리 용어로 다시 표현하자면 - 기본 ε을 쓰는 ADAM은 ε을 튜닝하는 ADAM과는 다른 옵티마이저라고 하면 - 우리 결과와 모순되지 않아. 그리고 그들이 실무자들에게 가장 흥미로운 형태의 ADAM은 고려하지 않았다는 게 명확해져.

각주:
¹ 오차 범위: 통계적 불확실성을 나타내는 범위
² 하이퍼파라미터: 학습 전에 설정하는 모델의 설정값들
³ 가중치 감쇠(weight decay): 오버피팅을 막기 위한 정규화 기법

<img width="897" alt="image" src="https://github.com/user-attachments/assets/d112e439-b9b6-4358-b697-2cdc25b6af11">

Figure 4

"더 많은 하이퍼파라미터를 튜닝하면 Schneider 등(2019)의 옵티마이저 순위가 포함 관계와 일관된 순위로 바뀌어.
각 워크로드의 가장 왼쪽 열은 Schneider 등(2019)의 순위를 재현한 거고¹, 나머지 열들은 점점 더 일반적인 탐색 공간에 대해 튜닝한 결과야². 모든 열에서 우리의 무작위 탐색 튜닝 프로토콜을 사용했어³."
[그래프 설명]

두 개의 워크로드(CIFAR-100의 CNN과 War and Peace의 LSTM)에 대한 결과를 보여줌
각 그래프는 서로 다른 튜닝 전략에 따른 성능을 비교
왼쪽에서 오른쪽으로 갈수록 더 많은 하이퍼파라미터를 튜닝
최종적으로 더 일반적인 옵티마이저(Adam)가 더 나은 성능을 보임


각주:
¹ Schneider 등은 학습률만 튜닝했고, 이 경우 SGD나 Momentum이 더 좋아 보였음
² "더 일반적인 탐색 공간"이란 더 많은 하이퍼파라미터를 튜닝 대상으로 포함했다는 의미
³ 무작위 탐색이 격자 탐색보다 더 효율적이라고 알려져 있음
이 그래프도 역시 하이퍼파라미터 튜닝의 범위가 옵티마이저 성능 비교에 얼마나 큰 영향을 미치는지 보여주는 결정적인 증거야. 충분히 튜닝하면 더 일반적인 옵티마이저가 항상 더 좋은(또는 같은) 성능을 보인다는 논문의 주장을 뒷받침하고 있지.

### 5. 결론

Wilson 등(2017)과 Schneider 등(2019)의 최근 노력에서 영감을 받아, 우리는 딥러닝에서 옵티마이저 선택 과정을 자세히 실증적으로 특성화하려고 했어.
우리의 핵심 발견은 옵티마이저들 간의 포함 관계가 실제로도 의미가 있다는 거야. 딥러닝에서 일반적인 규모로 현실적인 프로토콜 하에서 사용 가능한 모든 하이퍼파라미터를 튜닝할 때, 더 일반적인 옵티마이저가 자신의 특수한 경우보다 성능이 떨어지는 일은 절대 없다는 걸 발견했어.
특히, 우리가 가장 철저한 튜닝 프로토콜을 적용했을 때 RMSPROP, ADAM, NADAM이 SGD, NESTEROV, MOMENTUM보다 성능이 떨어지는 일은 절대 없었어.
서로를 근사할 수 없는 옵티마이저들을 비교할 때는 일관된 경향을 발견하지 못했어. 또 옵티마이저 순위에서 통계적으로 유의미한 차이가 없는 워크로드들도 있었고.
우리 실험에는 몇 가지 중요한 한계가 있어서, 결과를 과잉 일반화하지 않도록 조심해야 해. 첫 번째 주요 주의사항은 우리가 배치 크기를 바꾸는 효과는 측정하지 않았다는 거야.
최근의 실증 연구들(Shallue 등, 2019; Zhang 등, 2019)은 배치 크기를 늘리면 다른 옵티마이저들 간의 학습 시간 차이가 증가할 수 있다는 걸 보여줬어. SGD에서 MOMENTUM으로의 차이(Shallue 등, 2019)와 MOMENTUM에서 ADAM으로의 차이(Zhang 등, 2019)가 배치 크기가 커질수록 증가한다고 해.

그래도 우리가 사용한 것과 비슷한 튜닝 프로토콜에서는 어떤 배치 크기에서든 포함 관계가 예측력을 가질 거라고 강하게 의심해.
우리 결과의 두 번째 중요한 주의사항은 이게 불가피하게 우리가 고려한 튜닝 프로토콜과 워크로드들에 의존한다는 거야. 우리가 현실적인 실험을 하려고 최선을 다했지만, 우리의 자세한 발견들은 비슷한 프로토콜 하의 비슷한 워크로드들에 대해서만 적용될 것으로 예상해야 해.
구체적으로:

- 수십에서 수백 번의 준무작위 튜닝 시도
- 하이퍼큐브 탐색 공간¹
- 우리가 특정하게 파라미터화한 학습률 스케줄

근데 이런 주의사항들이 오히려 우리의 핵심 포인트를 강화해: 신경망 옵티마이저들의 모든 실증적 비교는 하이퍼파라미터 튜닝 프로토콜에 엄청나게 의존해. 아마 우리가 모델 아키텍처 간의 비교에서 익숙한 것보다 훨씬 더 많이.
만약 우리 발견에서 "모범 사례"를 뽑자면, 다음을 제안할 수 있어:

1. 코드를 수십 번 이상 실행할 여유가 있다면, 유명한 adaptive gradient 방법들의 모든 하이퍼파라미터를 다 튜닝해야 해.
2. 두 개의 하이퍼파라미터가 두 다른 업데이트 규칙에서 비슷한 역할을 한다고 해서 비슷한 값을 가져야 한다는 건 아니야. 최적화 하이퍼파라미터들은 서로 연결되어 있는 경향이 있고, 하나의 최적값은 다른 것들이 어떻게 설정되었는지에 따라 달라질 수 있어.

각주:
¹ 하이퍼큐브 탐색 공간: 각 하이퍼파라미터가 특정 범위 내에서 독립적으로 변할 수 있는 직육면체 모양의 탐색 공간

3. 우리 결과는 Adam의 ε이 문제에 따라 다르다는 걸 확인해줬어. 그래서 ε=10⁻⁸로 고정하는 실험 연구들은 그런 선택의 정당성을 입증해야 해.
4. 논문들의 옵티마이저 비교 결과는 의심해봐야 해. 특히 어떤 옵티마이저가 자기의 특수한 경우보다 성능이 안 좋다고 하면 더더욱. 우리가 옵티마이저들을 비교할 때는:

- 탐색 공간을 보고해야 하고
- 어떤 하이퍼파라미터를 튜닝했는지 결과 해석할 때 강조해야 해.



니네가 이 논문 읽으면서 기억해야 할 가장 중요한 메시지는:

- 옵티마이저 포함 관계가 현실에서도 실제로 중요하다
- 하이퍼파라미터 튜닝이 어떤 옵티마이저가 더 좋은지 판단하는 데 엄청난 영향을 미친다
- 지금까지 많은 연구들이 제한된 하이퍼파라미터 튜닝으로 인해 잘못된 결론을 내렸을 수 있다
- Adam이나 다른 적응형 옵티마이저들이 SGD나 Momentum보다 성능이 안 좋다는 건 잘못된 통념이었다¹


각주:
¹ 이전 연구들이 이런 결론을 내린 건 하이퍼파라미터 튜닝을 충분히 하지 않았기 때문

² 적응형 옵티마이저(Adaptive optimizers): Adam, RMSprop같이 학습률을 자동으로 조절하는 옵티마이저들
이게 논문의 마지막 부분이고 핵심적인 결론을 정리한 거야. 이 논문을 통해서 우리가 딥러닝 옵티마이저에 대해 가지고 있던 많은 통념이 잘못됐다는 걸 알 수 있어. 특히 "Adam이 SGD보다 성능이 안 좋다"는 널리 퍼진 믿음이 단순히 불충분한 실험 설계 때문이었다는 게 매우 흥미로워.
이 논문의 결과는 실제 딥러닝 모델을 학습시키는 사람들에게 매우 실용적인 가이드라인을 제공하고 있어. 특히 하이퍼파라미터 튜닝의 중요성과 더 일반적인 옵티마이저들(Adam, NAdam 등)이 제대로만 튜닝하면 항상 더 좋은(또는 최소한 같은) 성능을 낼 수 있다는 점을 강조하고 있지.
