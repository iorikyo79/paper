## 에폭(epoch), 배치(batch), 반복 횟수(iteration)

### 1. 에폭(Epoch):

에폭은 전체 학습 데이터를 한 번 완전히 순회하는 것을 의미합니다.
즉, 모든 학습 데이터를 모델에 한 번씩 입력하여 학습을 진행하는 과정이 한 에폭입니다.
일반적으로 모델을 학습할 때는 여러 에폭을 반복하여 모델을 업데이트합니다.


### 2. 배치(Batch):

현실적으로 대량의 학습 데이터를 한 번에 모두 모델에 입력하는 것은 메모리 제한 등의 이유로 어려울 수 있습니다.
따라서 전체 학습 데이터를 작은 묶음으로 나누어 학습을 진행하게 되는데, 이 작은 묶음을 배치라고 합니다.
예를 들어, 10,000개의 학습 데이터가 있고 배치 크기를 32로 설정한다면, 312개의 배치(10,000 / 32 ≈ 312)로 나누어 학습을 진행하게 됩니다.


### 3. 반복 횟수(Iteration):

반복 횟수는 한 에폭 내에서 배치 학습을 몇 번 수행하는지를 나타냅니다.
앞선 예시에서 10,000개의 학습 데이터를 배치 크기 32로 나누면 312개의 배치가 생성되므로, 한 에폭 내에서 312번의 반복 학습이 이루어집니다.
반복 횟수는 iter_num 변수로 표현되며, 첫 번째 배치의 인덱스는 0부터 시작합니다.


학습 과정에서는 각 배치마다 손실(loss)을 계산하게 됩니다. 한 에폭 동안 모든 배치에 대해 손실을 누적하여 합산한 값이 epoch_loss입니다. 에폭이 끝날 때, epoch_loss를 해당 에폭의 총 배치 수로 나누어주면 해당 에폭의 평균 손실을 얻을 수 있습니다.
예를 들어, 한 에폭 동안 다음과 같이 배치별 손실이 계산되었다고 가정해보겠습니다:

배치 1의 손실: 10
배치 2의 손실: 8
배치 3의 손실: 6
...
배치 312의 손실: 4

이 경우 epoch_loss는 모든 배치의 손실을 합산한 값이 됩니다. 에폭이 끝날 때, epoch_loss를 총 배치 수인 312로 나누어주면 해당 에폭의 평균 손실을 구할 수 있습니다.
따라서 학습 과정에서는 다음과 같은 단계를 거치게 됩니다:

전체 학습 데이터를 배치 단위로 나눕니다.
한 에폭 동안 모든 배치에 대해 학습을 진행하며, 각 배치의 손실을 누적하여 합산합니다.
에폭이 끝날 때, 누적된 손실을 총 배치 수로 나누어 평균 손실을 계산합니다.
여러 에폭을 반복하며 모델을 업데이트합니다.

이렇게 에폭, 배치, 반복 횟수의 개념을 이해하고 활용하면 대량의 학습 데이터를 효과적으로 처리하고 모델을 학습할 수 있습니다.

### 4. 학습률 스케줄러(Learning Rate Scheduler):

학습률(learning rate)은 모델의 가중치를 업데이트할 때 사용되는 중요한 하이퍼파라미터입니다.
학습률이 너무 크면 모델이 불안정해지거나 수렴하지 않을 수 있고, 너무 작으면 학습 속도가 느려질 수 있습니다.
학습률 스케줄러는 학습 과정에서 학습률을 동적으로 조정하여 최적의 학습을 가능하게 합니다.
일반적으로 학습이 진행됨에 따라 학습률을 점진적으로 감소시키는 방법이 많이 사용됩니다.


### 5. 스케줄러 스텝(Scheduler Step):

학습률 스케줄러는 지정된 기준에 따라 학습률을 조정합니다.
스케줄러 스텝은 학습률을 업데이트하는 시점을 결정하는 역할을 합니다.
일반적으로 에폭 단위로 스케줄러 스텝을 호출하여 학습률을 조정합니다.


scheduler.step(epoch_loss / len(dataloader_train)) 코드 설명:

이 코드는 에폭이 끝날 때마다 호출되어 학습률을 조정합니다.
epoch_loss는 해당 에폭의 총 손실을 나타내고, len(dataloader_train)은 학습 데이터의 총 배치 수를 나타냅니다.
epoch_loss / len(dataloader_train)은 해당 에폭의 평균 손실을 계산한 것입니다.
이 평균 손실 값을 스케줄러 스텝의 인자로 전달하여 학습률을 조정합니다.


### 6. 평균 손실을 사용하는 이유:

일부 학습률 스케줄러는 에폭의 손실 값을 기준으로 학습률을 조정합니다.
예를 들어, ReduceLROnPlateau 스케줄러는 지정된 에폭 동안 손실 값의 개선이 없으면 학습률을 감소시킵니다.
이때 개별 배치의 손실 값보다는 에폭 전체의 평균 손실을 사용하는 것이 더 안정적이고 대표성 있는 지표가 됩니다.
따라서 scheduler.step()에 에폭의 평균 손실을 전달하여 학습률을 조정하는 것이 일반적입니다.



이러한 학습률 스케줄러의 사용은 모델 학습의 효율성과 성능을 향상시키는 데 도움을 줍니다. 학습 초기에는 상대적으로 큰 학습률로 빠르게 수렴하고, 학습이 진행될수록 학습률을 점진적으로 감소시켜 미세 조정을 수행할 수 있습니다.
따라서 scheduler.step(epoch_loss / len(dataloader_train))은 에폭이 끝날 때마다 해당 에폭의 평균 손실을 기준으로 학습률을 조정하는 역할을 합니다. 이를 통해 모델 학습의 안정성과 수렴 속도를 개선할 수 있습니다.
