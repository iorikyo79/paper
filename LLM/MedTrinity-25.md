# MedTrinity-25M: 다중 세분화 주석이 포함된 대규모 다중 모달 의료 데이터셋

## 초록

이 논문에서는 MedTrinity-25M이라는 개쩌는 의료용 다중 모달 데이터셋을 소개합니다. 이 녀석은 10가지 모달리티에 걸쳐 2500만 개가 넘는 이미지를 포함하고 있는데, 65개 이상의 질병에 대해 여러 단계의 주석이 달려 있습니다. 이 풍성한 주석들은 질병/병변 유형, 모달리티, 특정 영역 설명, 영역 간 관계 같은 전반적인 텍스트 정보뿐만 아니라 관심 영역(ROI)에 대한 자세한 로컬 주석(경계 상자, 분할 마스크 등)도 포함하고 있어요.
기존 방식들은 이미지-텍스트 쌍의 가용성에 제한이 있었지만, 우리는 처음으로 자동화된 파이프라인을 개발해서 짝 지어진 텍스트 설명 없이도 다중 세분화된 시각 및 텍스트 주석(이미지-ROI-설명 삼중항 형태)을 생성할 수 있게 만들었습니다.
구체적으로, 90개가 넘는 다양한 출처에서 데이터를 수집하고 전처리한 다음, 도메인 특화 전문가 모델을 사용해 비정상 영역과 관련된 ROI를 식별했어요. 그런 다음 종합적인 지식 베이스를 구축하고 다중 모달 대규모 언어 모델을 프롬프트해서 식별된 ROI를 가이드로 사용하는 검색 증강 생성을 수행했더니, 결과적으로 다중 세분화된 텍스트 설명이 튀어나왔습니다.
기존 데이터셋과 비교하면, MedTrinity-25M은 가장 풍부한 주석을 제공하여 캡션 생성 및 보고서 생성과 같은 다중 모달 작업은 물론 분류 및 분할 같은 비전 중심 작업을 포괄적으로 지원합니다. MedTrinity-25M에서 사전 학습한 우리 모델은 VQA-RAD와 PathVQA에서 최고 성능을 달성했는데, 다중 모달 대규모 언어 모델과 다른 대표적인 최첨단 접근법들을 모조리 뛰어넘었습니다.
이 데이터셋은 대규모 다중 모달 의료 AI 모델의 사전 학습을 지원하는 데 사용될 수 있어서, 의료 분야의 미래 기반 모델 개발에 크게 기여할 수 있을 거예요. 이 개쩌는 데이터셋은 https://yunfeixie233.github.io/MedTrinity-25M/에서 공개적으로 이용 가능합니다.

### 1. 소개

대규모 다중 모달 기반 모델들[1, 2, 3, 4, 5]은 자연 언어와 함께 복잡한 시각적 패턴을 이해하는 능력 덕분에 다양한 분야에서 놀라운 성공을 거뒀어요. 이런 성공에 힘입어 의료 비전-언어 작업에 이런 모델들을 적용하는 데 큰 관심이 쏠리고 있죠. 의료 이미지-텍스트 쌍으로 데이터셋을 구축하고 이를 통해 일반 도메인 모델을 파인튜닝하는 방식으로 일반 도메인 다중 모달 기반 모델의 의료 능력을 향상시키는 데 많은 진전이 있었습니다[6, 7, 8, 9, 10].
하지만 현재의 의료 데이터셋들은 몇 가지 한계가 있어요. 첫째, 이 데이터셋들은 의료 이미지 내 로컬 정보와 글로벌 정보 간의 상관관계를 보여주는 다중 세분화 주석이 부족해요. 의료 이미지에는 종종 특정 유형의 병변을 나타낼 수 있는 국소적인 비정상 질감이나 구조 같은 세부적인 단서들이 포함되어 있거든요. 따라서 다중 모달 모델은 이런 로컬 세부 사항에서 질병이나 병변 유형 같은 글로벌 정보를 추론할 수 있는 능력이 필요해요. 이런 데이터가 없으면 모델이 의료 이미지를 종합적으로 이해하는 능력이 제한됩니다. 게다가 현재의 데이터셋 구축 방법은 보고서나 캡션과 짝을 이룬 의료 이미지에 크게 의존하고 있어서 확장성이 제한되어 있어요.
이 논문에서는 위의 과제들을 해결하기 위해 짝을 이룬 텍스트 설명에 의존하지 않고 다중 모달 대규모 언어 모델(MLLM)을 사용하는 자동화된 데이터 구축 파이프라인을 제안합니다. 일반 목적 MLLM의 포괄적인 의료 지식 부족 문제를 해결하기 위해, 우리는 도메인 특화 전문가 접지 모델과 검색 증강 생성(RAG)을 활용해 관련 의료 지식을 추출했어요. 그런 다음 MLLM에 프롬프트를 주어 식별된 관심 영역(ROI)을 기반으로 이 지식이 풍부해진 다중 세분화 시각 및 텍스트 주석을 생성하도록 했습니다.
우리는 이 파이프라인을 사용해 대규모의 짝 지어지지 않은 이미지를 포함한 수집된 데이터를 이미지-ROI-설명 삼중항으로 변환했어요. 이 삼중항들은 질병/병변 유형, 모달리티, 영역 간 관계 같은 글로벌 텍스트 정보뿐만 아니라 경계 상자, 분할 마스크, 영역별 텍스트 설명을 포함한 ROI에 대한 상세한 로컬 주석도 포함하는 다중 세분화 주석을 제공합니다. 제안된 파이프라인을 사용해 2500만 개 이상의 삼중항을 포함하는 대규모 다중 모달 다중 세분화 의료 데이터셋인 MedTrinity-25M을 만들었습니다. 우리가 아는 한, 이것은 지금까지 만들어진 가장 큰 의료 다중 모달 데이터셋입니다.

처음에 우리는 TCIA, Kaggle, Zenodo, Synapse 등 90개가 넘는 온라인 리소스에서 대량의 의료 데이터를 모았어요. 고품질의 수동 보고서와 짝을 이룬 소량의 이미지 외에도, 이 모은 데이터에는 두 가지 유형의 거친 의료 데이터가 포함되어 있습니다:

   - 상세한 텍스트 설명은 없지만 분할 마스크, 병변 경계 상자, 또는 단순히 질병 유형만 있는 이미지 데이터
   - 전체적인 모달리티나 질병 정보만 설명하고 로컬 영역에 대한 상세한 설명이 없는 대략적인 캡션과 짝을 이룬 이미지

대량의 거친 의료 데이터에서 다중 세분화 주석을 생성하기 위해, 우리는 먼저 전문가 접지 모델을 적용하여 질병이나 병변 패턴을 포함하는 ROI를 식별했어요. 그런 다음 온라인 말뭉치(예: PubMed)에서 포괄적인 지식 베이스를 구축하고 이미지 관련 의료 지식을 검색했습니다. 마지막으로, MLLM에 프롬프트를 주어 식별된 ROI의 안내에 따라 의료 지식을 통합하여 다중 세분화된 텍스트 설명을 생성하도록 했어요.


### 2. 관련 연구

의료 다중 모달 기반 모델: 시각적 특징을 이해하는 데 다중 모달 기반 모델이 효과적이라는 점 때문에, 최근 몇 년 동안 이런 모델들을 의료 비전-언어 작업에 적용하는 데 관심이 높아지고 있어요[11, 12, 9, 5]. 여러 논문에서 다양한 아키텍처를 가진 일반 도메인 다중 모달 기반 모델을 의료 도메인에 적응시키기 위해 의료 데이터셋에 대한 종단간 훈련을 시도했습니다.
예를 들어, Med-Flamingo[11]는 0.8M의 인터리브된 데이터와 1.6M의 짝지어진 의료 이미지-텍스트 데이터로 OpenFlamingo-9B[13]를 파인튜닝하여 의료 능력을 향상시켰어요. Med-PalM[12]은 약 1M의 의료 데이터 포인트를 사용해 PaLM-E[14]를 의료 도메인에 적응시켜 최첨단 모델들과 비교하여 경쟁력 있거나 더 뛰어난 성능을 보여줬죠. 또한, LLaVA-Med[9]는 두 단계의 종단간 시각적 지시 튜닝[1]을 사용해 의료 시각 질문 답변(VQA) 작업에서 놀라운 결과를 달성했어요. 비슷하게, Med-Gemini[15]는 장문형 질문 답변 데이터셋을 사용해 기본 Gemini[16]의 다중 모달 및 긴 맥락 능력을 강화했습니다.
이 모델들이 놀라운 성능을 보여줬지만, 여전히 훈련 데이터의 규모에 제한을 받고 있어요. 이전 연구[17]에서는 훈련 데이터의 규모를 키우면 대규모 다중 모달 기반 모델의 성능이 향상된다는 걸 보여줬습니다. 이 논문에서는 더 강력한 의료 다중 모달 기반 모델의 개발을 촉진하기 위해 대규모 의료 데이터셋을 구축하는 것을 목표로 하고 있어요.

의료를 위한 다중 모달 데이터셋: 포괄적인 의료 다중 모달 데이터셋을 구축하는 중요성이 상당한 주목을 받고 있어요[9, 18, 19, 7]. 몇몇 연구에서는 병리 전문가가 준비한 이미지와 그에 짝지어진 임상 보고서를 수집하려고 시도했는데[19, 7, 8], 이런 보고서들은 질병 유형과 그에 대한 추론을 포함한 이미지에 대한 포괄적인 설명을 제공해요. 예를 들어, MIMIC-CXR[8]은 65,379명의 환자에 대해 227,835개의 이미지를 포함하고 있는데, 각 이미지와 짝을 이룬 보고서에 병리학적 소견과 인상이 포함되어 있어요. 하지만 이런 보고서를 수동으로 만드는 건 시간도 많이 걸리고 비용도 많이 들어서, 이런 데이터셋의 규모가 제한될 수밖에 없어요.
PMC-OA[20]는 의학 논문에서 대량의 이미지-캡션 쌍을 추출해 데이터셋 규모를 165만 개 샘플로 확장하는 걸 목표로 했어요. 하지만 추출된 캡션은 수동 임상 보고서에 비해 덜 상세해서 다중 세분화된 주석이 부족해요. RadGenome-Chest CT[19]는 분할 마스크와 MLLM이 생성한 의료 보고서 같은 더 상세한 주석을 포함하고 있어요. 그럼에도 불구하고, 이 방법도 여전히 짝지어진 이미지-텍스트 데이터에 의존하고 있어서 확장성이 제한돼요.
이런 기존 방법들과 달리, 우리는 짝지어지지 않은 이미지에 대해 다중 세분화된 주석을 생성하는 최초의 자동화된 데이터 구축 파이프라인을 고안했고, 이를 통해 2500만 개의 데이터 샘플을 포함하는 포괄적인 다중 세분화 데이터셋을 달성했어요.

### 3. MedTrinity-25M 데이터셋

#### 3.1 데이터 삼중항

우리 데이터셋은 {이미지, ROI, 설명}의 삼중항으로 구성돼 있어요. 각 ROI는 비정상성과 연관되어 있고, 경계 상자나 분할 마스크로 표현되며, 이미지 내의 관련 영역을 지정해요. 각 이미지에 대해, 우리는 그림 2에 나와 있는 것처럼 질병/병변 유형, 모달리티, 영역별 설명, 영역 간 관계를 포함하는 다중 세분화된 텍스트 설명을 제공해요.
이미지: 우리는 소스 데이터셋의 원본 의료 이미지를 사용하며, TCIA, Kaggle, Zenodo, Synapse, Hugging Face, Grand Challenge, GitHub 등의 온라인 리소스와 CheXpert[7], DeepLesion[23] 같은 관련 의료 데이터셋 연구에서 의료 데이터셋을 광범위하게 수집했어요. 이 데이터셋들은 먼저 두 가지 유형으로 분류됐어요:

   - 로컬 주석을 포함하는 데이터셋: 예를 들어, 해당하는 방사선학 보고서가 있는 MIMIC-CXR[8]과 해당하는 캡션이 있는 PMC-OA[24]인데, 여기서 보고서나 캡션은 이미지의 특정 로컬 상태에 대한 분석을 제공해요. 또 다른 예로는 3D 이미지 분할 데이터셋인 BraTS2024[25]가 있는데, 이는 CT 스캔의 종양 영역을 마스크로 표시해요.
   - 글로벌 주석을 포함하는 데이터셋: 예를 들어, 이미지 분류 데이터셋인 ISIC2019[26]와 ISIC2020[27]인데, 이들의 분류 레이블은 조직 절편의 전반적인 병리학적 상태를 반영해요. 또 다른 예로는 CheXpert[7] 데이터셋이 있는데, 이는 각 흉부 X선에 대해 상세한 질병 유형 분류를 제공해요.

우리는 10개의 모달리티와 65개 이상의 질병에 걸쳐 25,001,668개의 샘플을 수집했어요. DICOM이나 NIfTI 형식으로 저장된 3D 볼륨 이미지의 경우, 우리는 각 2D 슬라이스를 PNG 형식으로 변환했어요. 이런 데이터셋의 추가 캡션과 마스크, 경계 상자 같은 주석은 아래와 같이 ROI와 해당 텍스트 설명을 구성하는 데 활용됐어요.

**ROI**: 각 이미지에 대해, ROI는 분할 마스크나 경계 상자를 사용해 강조돼요. 이 ROI들은 대부분 병변, 염증, 종양, 감염 또는 기타 잠재적 이상 같은 병리학적 소견을 포함하고 있어요. 이상이 없는 몇몇 경우에는, ROI가 일반적으로 이미지의 주요 물체나 장기를 가리키는데, 이에 대한 예시는 보충 자료에서 볼 수 있어요.

**텍스트 설명**: 각 이미지에 대한 텍스트 설명은 다양한 측면에서 상세한 정보를 제공해요. 이전의 의료 보고서 데이터셋[7, 8, 6]에서 볼 수 있는 구조화되지 않은 자유 형식의 설명이나 시각 QA 데이터셋[28, 22]과 캡션 데이터셋[18, 24]에서 볼 수 있는 단순한 짧은 문장과는 달리, 우리의 텍스트 설명은 다중 세분화되고 구조화되어 있어요.
먼저 이미지와 관련된 일반적인 속성을 설명하는데, 여기에는 이미지 모달리티, 묘사된 특정 장기, 제시된 질병 유형이 포함돼요. 그 다음에는 ROI 관련 정보가 제공되는데, 여기에는 ROI의 위치와 기저 병리를 나타내는 ROI 내의 비정상적 특징(예: 독특한 색상과 질감)이 포함돼요. 또한 ROI와 주변 영역 간의 비교도 제시되어 특징의 차이와 질병 진행 정도를 강조해요.
우리는 또한 그림 1에서 볼 수 있듯이, 우리 데이터셋의 다중 세분화된 텍스트 설명을 다른 일반적인 형태의 설명과 비교해 보여줘요. 우리의 텍스트 설명은 흉부 X선 데이터셋 MIMIC-CXR[21]의 방사선학 보고서, 시각 QA 데이터셋 SLAKE[22], 방사선학 객체 캡션 데이터셋 ROCO[18]보다 더 많은 속성을 가진 다중 세분화된 설명이에요.


#### 3.2 데이터 구축 파이프라인
주어진 의료 이미지에 대해, 우리는 MLLM을 활용해 해당하는 다중 세분화된 시각 및 텍스트 주석을 생성하는 것을 목표로 해요. 구체적으로, 그림 2에서 볼 수 있듯이, 우리의 파이프라인은 데이터 처리와 다중 세분화된 텍스트 설명 생성이라는 두 단계로 나눌 수 있어요.
데이터 처리 단계(3.2.1절)에서는 전문가 접지 모델과 검색 증강 생성(RAG)을 활용해 일반 목적 MLLM의 도메인 특화 지식 부족 문제를 해결해요. 이 단계는 세 가지 주요 단계를 포함해요:

   - 메타데이터 통합: 모달리티와 질병 유형 같은 기본적인 이미지 정보를 포함하는 대략적인 캡션을 생성해요.
   - ROI 위치 지정: 비정상 영역을 식별해요.
   - 의료 지식 검색: 관련된 세부적인 의료 정보를 추출해요.

처리된 데이터를 기반으로, 우리는 그 다음 MLLM에 프롬프트를 주어 다중 세분화된 텍스트 설명을 생성하도록 해서, 3.2.2절에서 자세히 설명하는 것처럼 세밀한 캡션을 만들어내요.

#### 3.2.1 데이터 처리

**메타데이터 통합을 통한 대략적인 캡션 생성**:
우리는 주어진 이미지에 대한 기본적인 정보를 제공하는 대략적인 캡션을 생성하는 것을 목표로 해요. 여기에는 모달리티, 장기 레이블, 질병 유형, 그리고 선택적으로 카메라 뷰와 장비 정보가 포함돼요. 이미지에서 직접 특징을 추출하는 대신, 우리는 데이터셋 메타데이터를 통합해 이런 캡션을 생성해요.
먼저 데이터셋에서 메타데이터를 추출한 다음, 고정된 규칙을 적용해 이 정보를 대략적인 캡션으로 통합해요. 예를 들어, QaTa-COV19 데이터셋의 이미지의 경우, 우리는 데이터셋의 부속 논문이나 문서에서 메타데이터를 추출하는데, 이는 COVID-19 흉부 X선 이미지로 구성되어 있다고 해요. 그 다음, 우리는 "폐에 COVID-19가 있는 흉부 X선 이미지"와 같은 대략적인 캡션을 구성해서 모달리티, 장기 유형, 질병 레이블을 강조해요. 만약 이미지에 방사선학적 소견 같은 추가 텍스트 정보가 포함되어 있다면, 이것도 통합해서 캡션의 풍부함을 높여요.
세밀한 캡션을 생성할 때 대략적인 캡션을 추가하는 것의 효과는 그림 3에 나와 있어요. 대략적인 캡션 없이는 MLLM이 질병을 인식하지 못하는 반면, "COVID-19"라는 질병 유형을 포함하는 대략적인 캡션을 MLLM에 제공하면 질병을 식별하고 분류할 수 있게 되어 추가 분석의 기반을 마련해요.

**ROI 위치 지정**:
우리는 이미지에서 관심 영역(ROI)을 찾기 위해 다양한 전략을 사용해요. 이미 분할 마스크나 경계 상자 같은 위치 주석이 포함된 데이터셋의 경우, 우리는 이런 기존 주석에서 ROI를 도출해요. 구체적으로, 경계 상자는 직접 ROI로 사용되고, 분할 마스크는 마스크를 덮는 가장 작은 경계 상자를 만들어 ROI로 변환해요.
이런 위치 주석이 없을 때는, 부록에 나열된 다양한 사전 훈련된 전문가 모델을 적용해 ROI를 생성해요. 텍스트-프롬프트 기반 접지 모델[29]의 경우, 우리는 대략적인 캡션의 질병 및 장기 정보를 텍스트 프롬프트로 사용해 모델이 특정 부분을 분할하도록 안내해요. 다양한 모달리티에서 다른 모델들로 생성된 ROI의 예시는 그림 6에서 볼 수 있어요. X선과 MRI 스캔 같이 z축에서 본 모달리티의 경우, 우리의 ROI 위치 지정은 인체에 상대적인 좌표계를 사용하므로, 이미지 표현에서 좌우가 뒤바뀌게 돼요.
ROI 없이는 원래 설명이 이미지의 간단한 전체적 분석으로 제한돼요. 하지만 ROI가 있으면, MLLM은 ROI의 더 자세한 로컬 분석을 수행하고 병변 ROI가 주변 정상 영역에 미치는 영향을 평가할 수 있어요. 이는 그림 4에서 보여지고 있어요.
처리된 데이터를 기반으로, 우리는 그 다음 MLLM에 프롬프트를 주어 다중 세분화된 텍스트 설명을 생성하도록 해서, 3.2.2절에서 자세히 설명하는 것처럼 세밀한 캡션을 만들어내요.

**의료 지식 검색**:
일반 목적 MLLM은 종종 전문적인 의료 용어와 전문적인 표현이 부족한 내용을 생성해요. 이 문제를 해결하기 위해, 우리는 MedRAG[32]의 접근 방식을 따라 의료 지식 데이터베이스를 구축했어요. 우리는 세 가지 주요 말뭉치를 수집했어요: 생물의학 지식을 위한 PubMed, 임상 의사 결정 지원을 위한 StatPearls, 그리고 도메인 특화 지식을 위한 의학 교과서[33]예요.
이 말뭉치들을 짧은 스니펫으로 분할하고 Med-CPT[34]의 텍스트 인코더를 사용해 고차원 벡터로 인코딩했어요. 그런 다음 이 벡터들을 Faiss[35]를 사용해 효율적인 검색을 위해 최적화된 특수 벡터 지식 베이스로 색인화했어요.
주어진 이미지에 대해, 우리는 메타데이터 통합을 통해 생성된 대략적인 캡션을 사용해 관련 의료 지식을 검색해요. 구체적으로, 우리는 질병 및 장기 분류를 포함한 대략적인 캡션을 Med-CPT 텍스트 인코더를 사용해 벡터로 인코딩해요. 그런 다음 의료 벡터 데이터베이스에서 벡터 유사도 검색을 수행해 쿼리와 의미적으로 일치하는 상위 8개의 의료 지식 스니펫을 검색해요. 이 스니펫들이 이미지와 짝을 이루는 외부 의료 지식을 제공해요.
외부 의료 지식을 통합하는 것의 효과를 보여주는 정성적 예시는 그림 7에 나와 있어요. COVID-19 관련 의료 지식에 접근함으로써, MLLM은 의료 용어를 표준화하고 의학 문헌에 설명된 질병 진행 과정을 바탕으로 진단을 정제할 수 있어요.


#### 3.2.2 다중 세분화된 텍스트 설명 생성

데이터 처리 후, 포괄적인 프롬프트를 사용해 MLLM이 다중 세분화된 설명을 생성하도록 안내해요. 프롬프트 템플릿은 MLLM에 지시하는 질문과 함께 세 단계의 계층적 프레임워크로 구성돼요:

   - 이미지의 모든 세부 사항을 포착하는 전체적인 설명
   - 잠재적으로 비정상적일 수 있는 특정 ROI에 대한 로컬 중심의 분석
   - 로컬 이상이 전체 장기에 미치는 영향을 이해하기 위한 로컬-글로벌 속성 간의 상호작용 검사

상세한 프롬프트 템플릿은 보충 자료에 제시되어 있어요.
MLLM이 자체 훈련 데이터에 본질적으로 존재하지 않는 관련 의료 정보에 의해 안내되도록 하기 위해, 우리는 처리된 데이터(대략적인 캡션, ROI, 검색된 의료 지식)를 프롬프트에 통합해요. 구체적으로, 전체 정보에 대해서는 대략적인 캡션을 프롬프트에 직접 통합해요. 로컬 정보에 대해서는, 이미지의 ROI를 그 좌표와 이미지 내 면적 비율을 기반으로 텍스트 설명으로 변환해요. 이런 텍스트 설명의 예시는 그림 6에 나와 있는데, "좌측-중앙"과 "면적 비율: 1.2%" 같은 용어를 사용해요.
ROI 내의 용어와 진단을 정제하기 위해, 특정 질병에 대한 관련 의료 지식을 프롬프트에 통합해요. 이 지식을 단순히 삽입하는 대신, 우리는 MLLM에게 분석이 필요한 ROI와 관련 지식을 식별하고 정렬하도록 지시해요.

**MLLM 선택**
우리는 먼저 제공된 의료 대략 캡션, ROI, 의료 지식을 사용해 GPT-4V에 프롬프트를 주어 20만 개의 샘플 서브셋을 생성했어요. 이 서브셋은 전체 2500만 데이터셋과 비슷한 모달리티 및 장기 분포를 유지하고 있어요. 이 서브셋을 만든 목적은 의료 지식 안내 MLLM을 보정해서 우리 텍스트에 지정된 포맷 지침을 따르도록 하기 위해서예요.
그 다음, 우리는 최첨단 의료 MLLM인 LLAVA-Med[9]를 기반으로 한 LLaVA-Med Captioner 모델을 사용했어요. 이 모델을 더 개선하기 위해, 우리는 최신 LLaMA3[36]를 활용해 언어 능력을 향상시키고, 다중 스케일 특징 추출[37]을 통합해 시각 능력을 개선했어요. LLaVA-Med Captioner는 의료 다중 모달 데이터에 대해 지속적으로 훈련되고, 우리의 다중 세분화 주석을 사용해 미세 조정되어 전문화된 의료 모델이 됐어요.
미세 조정 후, 우리는 이 전문화된 모델을 사용해 전체 데이터셋에 대해 다중 세분화된 텍스트 설명을 생성했고, 그 결과 2500만 개의 이미지-ROI-설명 삼중항이 만들어졌어요. 미세 조정 과정은 GPT-4V의 고급 언어 구성 능력을 활용해 세밀한 캡션을 위한 효과적인 템플릿을 제공하고, 우리 모델은 이를 사용해 세밀한 캡션의 형식을 학습해요. 그 결과, 우리 모델은 그림 8에서 볼 수 있듯이 GPT-4V보다 더 자세한 설명을 생성해요. 우리는 또한 보충 자료의 부록 B에서 상세한 정량적 비교를 보여주고 있어요.


#### 3.3 데이터셋 분석
**다양성**: 우리 데이터셋은 인체의 다양한 해부학적 구조에 걸쳐 65개 이상의 질병을 포함하는 10가지 광범위한 이미징 모달리티를 포함하고 있어요. MedTrinity-25M의 해부학적 및 생물학적 구조 분포는 그림 9b에 나와 있어요. 한편, 각 모달리티의 데이터셋 샘플 수는 그림 9a에 나와 있는데, 각각 100만 개 이상의 샘플을 가진 일반적인 모달리티(CT, MRI, X-ray)부터 희귀한 모달리티(초음파, 피부경)까지 다양해요. 이는 초음파와 피부경 샘플이 단 수천 개밖에 없는 SA-Med2D-20M[38] 같은 다른 대규모 데이터셋에 비해 훨씬 더 균형 잡힌 분포를 보여주고 있어요.
**규모**: 그림 9c는 우리 데이터셋의 규모를 보여주는데, 이는 이전 데이터셋들보다 상당히 큽니다. 우리가 아는 한, 이는 지금까지 가장 큰 오픈소스 다중 모달 다중 세분화 의료 데이터셋이에요.
**질병**: MedTrinity-25M 구축에 관련된 데이터셋들은 주로 질병 진단과 의료 발견에 중점을 두고 있어요. MedTrinity-25M에서 질병은 자유 형식의 텍스트로 제공돼요. 같은 질병이 다른 용어로 언급될 수 있어서, 정교한 식별과 분석이 가능해요. 그림 9d는 우리 데이터셋에서 질병과 관련해 자주 사용되는 단어들을 보여주고 있어요.

**풍부성**: 우리는 생성된 다중 세분화 주석의 풍부함을 다른 의료 데이터셋과 비교하기 위해 정량적 분석과 정성적 예시를 모두 제공해요. 정성적 예시는 그림 1에 나와 있는데, 우리의 텍스트 설명이 흉부 X선 데이터셋 MIMIC-CXR[21]의 방사선학 보고서, 시각 QA 데이터셋 SLAKE[22], 그리고 방사선학 객체 캡션 데이터셋 ROCO[18]보다 더 많은 속성을 가진 다중 세분화된 설명이라는 걸 보여줘요.
우리 데이터의 다중 세분화를 보여주기 위해, 그림 10에 나와 있듯이 우리 데이터셋인 MedTrinity-25M의 텍스트 설명 평균 단어 수를 다른 의료 데이터셋과 비교했어요. 우리 데이터셋의 단어 수가 상당히 더 많은데, 이는 더 큰 풍부함을 나타내요.
**인간과의 정렬**: 생성된 다중 세분화 주석의 유효성과 품질을 평가하기 위해, 우리는 이를 원래의 인간 주석과 비교해 정렬 정도를 평가했어요(인간 주석이 있는 샘플의 경우).
생성된 세밀한 캡션이 자유 텍스트 방사선학 보고서 및 질문-답변 쌍과 상당히 다른 구조화된 설명을 포함하고 있기 때문에, 우리는 GPT-4V의 시각 및 언어 이해 능력을 활용했어요. 문장 구조나 구성의 정확한 정렬에 초점을 맞추기보다는, GPT-4V는 의료 사실과 진단의 정확성을 기반으로 정렬을 평가했어요.
구체적으로, 생성된 세밀한 캡션의 구조는 의료 이미지를 특징짓는 다섯 가지 주요 속성으로 구성돼요: 모달리티, 구조 감지, ROI 분석, 병변 질감, 로컬-글로벌 관계. 생성된 데이터를 평가하기 위해, 우리는 GPT-4V가 이 다섯 가지 속성을 기반으로 인간 주석과 상세한 비교를 수행하도록 했어요. 각 속성은 0에서 2점 사이의 점수로 평가되었고, 최대 가능 총점은 10점이에요.
우리는 SLAKE[22]와 MIMIC-CXR[21]에 대해 정렬 연구를 수행했는데, 인간 주석과 비교해 정렬 점수를 평가하기 위해 50개의 샘플을 무작위로 선택했어요. 표 2에서 볼 수 있듯이, SLAKE와 MIMIC-CXR에 대한 정렬 점수는 각각 8.2와 8.9였어요. 모달리티, 구조 감지, ROI 분석 기준은 거의 완벽한 점수를 달성했는데, 이는 인간 주석과 비교했을 때 생성된 데이터의 유효성과 정확성을 보여줘요. GPT-4V가 평가한 완벽한 정렬 점수 결과의 예시는 그림 11에 나와 있어요. 이 예시들에서 GPT-4V는 다섯 가지 기준 모두에서 인간 주석과 완전히 정렬되어 완벽한 정렬 점수를 얻었어요.
정렬 점수를 평가하기 위해 GPT-4V에 사용된 프롬프트는 보충 자료의 그림 14에 나와 있어요.


### 4. LLaVA-Med++: MedTrinity-25M을 사용한 실험적 훈련

우리 데이터셋의 유효성을 더 입증하기 위해, 우리는 우리 데이터셋으로 훈련한 경우와 그렇지 않은 경우의 LLaVA-Med++의 성능을 비교해요. 우리는 모델이 상세한 시각 및 언어 표현을 학습해야 하는 시각 질문 답변(VQA) 작업을 평가 작업으로 선택했어요. 우리는 세 가지 생물의학 VQA 데이터셋에서 우리 모델의 성능을 평가했어요: VQA-RAD[42], SLAKE[22], PathVQA[45].

우리는 먼저 LLaVA-Med[9]의 방법론을 사용해 LLaVA-Med++를 사전 훈련시켜 기준선으로 삼았어요. 그 다음, 각 VQA 데이터셋 평가를 위해, 우리는 해당하는 MedTrinity-25M 서브셋에서 우리 모델을 추가로 사전 훈련시켜 다중 세분화 정렬을 달성했어요. 그런 다음 모델을 VQA 데이터셋에서 3 에폭 동안 미세 조정했고, 성능 결과는 표 3에 제시되어 있어요.
비교 실험으로, MedTrinity-25M에서의 사전 훈련 없이 다른 모든 설정을 유지한 채 실험을 수행했어요. 결과는 LLaVA-Med++가 세 개의 VQA 벤치마크 중 두 개에서 최첨단 성능을 달성했고, 나머지 하나에서는 3위를 차지했음을 명확히 보여줘요.
MedTrinity-25M에서 사전 훈련한 경우, 사전 훈련 없이 훈련한 모델에 비해 VQA-RAD에서 약 10.75%, SLAKE에서 6.1%, PathVQA에서 13.25%의 성능 향상을 보여줬어요. 이러한 개선은 MedTrinity-25M에서의 사전 훈련이 다운스트림 다중 모달 의료 작업, 특히 시각 질문 답변에 효과적임을 강조해요.


### 5. 결론

이 논문에서는 MedTrinity-25M을 소개했어요. 이는 90개 이상의 온라인 리소스에서 얻은 2500만 개 이상의 이미지-ROI-설명 삼중항으로 구성된 대규모 다중 모달 의료 데이터셋으로, 10가지 모달리티에 걸쳐 65개 이상의 질병을 다루고 있어요.
기존의 이미지-텍스트 쌍에 의존하는 데이터셋 구축 방법과 달리, 우리는 짝지어지지 않은 이미지 입력에서 다중 세분화된 시각 및 텍스트 주석을 생성하여 다중 모달 데이터의 규모를 확장하는 최초의 자동화된 파이프라인을 개발했어요. 이 과정에서 전문가 접지 모델, 검색 증강 생성 기술, 그리고 고급 MLLM을 활용했죠.
MedTrinity-25M의 풍부한 주석은 캡션 생성, 보고서 생성, 분류, 분할과 같은 다양한 다중 모달 작업을 지원할 수 있는 잠재력을 가지고 있어요. 또한 다중 모달 의료 AI 모델의 대규모 사전 훈련을 촉진할 수 있을 거예요.
이 데이터셋은 의료 AI 연구와 응용 분야에서 중요한 자원이 될 것 같아요. 앞으로 이를 활용한 더 정교한 의료 AI 모델들이 개발되면 진단의 정확성이 높아지고 의료 서비스의 질이 향상될 수 있을 것 같네요. 정말 기대되는 연구 결과입니다!
