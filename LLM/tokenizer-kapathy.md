https://www.youtube.com/watch?v=zduSFxRajkE

## <안드레이 카파시의 토크나이저>
1. 토크나이제이션의 중요성
 - 토크나이제이션이 LLM 성능에 미치는 영향
 - 토크나이제이션의 기본 개념 및 동작 원리
2. Byte Pair Encoding(BPE) 알고리즘
 - BPE 알고리즘의 작동 방식
 - BPE를 활용한 토크나이저 구현
 - 문자 수준의 토크나이제이션 vs BPE 토크나이제이션
3. 실전 GPT 토크나이저 분석
 - GPT-2와 GPT-4 토크나이저 비교
 - 정규 표현식을 이용한 토큰 분리
 - 특수 토큰의 처리
4. Sentence Piece 라이브러리
 - Sentence Piece와 Tick Token의 차이점
 - Sentence Piece의 장단점
 - Sentence Piece 라이브러리 사용법
5. 토크나이저 성능 최적화 팁
 - Vocabulary 크기 설정
 - 희귀 토큰 처리 (Byte Fallback)
 - 토큰 density와 compression ratio
 - 다국어 데이터셋에 대한 토크나이제이션
6. 토크나이제이션 실수례와 교훈
 - 불완전한 토큰으로 인한 모델 오작동
 - Solid Gold Magikarp 사례와 시사점
 - 데이터 포맷에 따른 토큰 효율성 (JSON vs YAML)
7. 카파시의 조언과 전망
 - 최신 토크나이제이션 연구 동향
 - 이상적인 토크나이저의 조건
 - 토크나이제이션의 한계 극복을 위한 제언
부록. MinBPE 토크나이저 실습
 - MinBPE 라이브러리 소개
 - 단계별 실습 가이드

### 1. 토크나이제이션의 중요성
야, 너도 LLM 돌려본 적 있제? 근데 결과가 좀 이상했던 적 없어?
야이, 난 LLM 토크나이저 때문에 삽질 엄청했다고. 토크나이저가 뭔 역할 하는지 안 까먹었지?

LLM이 텍스트를 처리할 때 걍 글자를 그냥 바로 입력으로 쓸 순 없거든.
왜냐면 컴터는 숫자밖에 모르잖아. 그래서 텍스트를 토큰이라는 숫자로 바꿔주는 게 필요한데, 이걸 토크나이제이션이라 그러지.

근데 말야, 이 토크나이제이션을 어떻게 하느냐에 따라서 LLM 성능이 왔다갔다 한다고.
가령 영어랑 한국어 텍스트 길이가 비슷해도 토큰 개수는 한국어가 더 많아.
그럼 LLM 입장에서는 한국어 처리가 더 버거울 수밖에.

숫자 연산도 토크나이제이션 때문에 제대로 안 되는 경우도 있대.
숫자를 이상하게 쪼개서 토큰으로 만들면 LLM이 큰 숫자 계산을 못 하는 거지.

그니까 토크나이제이션이 LLM한테는 진짜 중요한 문제란 말이야.
토크나이저 잘못 만들면 LLM이 아무리 잘 학습해도 소용없다고.

안드레이 카파시도 GPT 만들 때 토크나이제이션 때문에 엄청 고생했대.
걔가 오죽하면 "토크나이제이션은 내가 제일 싫어하는 부분이지만 중요하니까 꼭 이해해야 해"라고 했겠어.

그래서 이번 책에서는 카파시의 토크나이저 노하우를 낱낱이 파헤쳐볼 거야.
니가 나중에 LLM 만들 때 토크나이저로 고생 안 하게 말야.

자, 그럼 가보자고. BPE 알고리즘부터 시작이다!

### 2. Byte Pair Encoding(BPE) 알고리즘
BPE 알고리즘, 들어는 봤제? 토크나이저 만들 때 쓰는 핵심 알고리즘이라고.
근데 이게 좀 복잡해서 머리 아픈 건 사실이야. 그래도 꼭 이해해야 돼!

일단 BPE가 뭔지부터 알아보자. 쉽게 말해서 BPE는 텍스트를 토큰으로 쪼개는 방법 중 하나야.
처음에는 텍스트를 문자 단위로 쪼갠 다음에, 자주 나오는 문자 쌍을 하나의 토큰으로 합치는 걸 반복하는 거지.

예를 들어서 "aaabdaaabac"라는 텍스트가 있다고 치자.
처음에는 'a', 'a', 'a', 'b', 'd', 'a', 'a', 'a', 'b', 'a', 'c'로 쪼개.
그 다음에 가장 많이 나오는 문자 쌍을 찾아. 여기서는 'aa'겠지?
그럼 'aa'를 하나의 토큰으로 합쳐. 이걸 'Z'라고 하자.
그럼 이제 텍스트는 'Z', 'a', 'b', 'd', 'Z', 'a', 'b', 'a', 'c'가 돼.

이런 식으로 자주 나오는 문자 쌍을 계속 합쳐 나가는 거야.
'Z', 'a', 'b' 이런 게 많으면 또 합치고.
이렇게 반복하다 보면 자연스럽게 텍스트를 토큰으로 쪼갤 수 있어.

그럼 이제 실제로 BPE로 토크나이저를 만드는 법을 알아보자.
일단 UTF-8로 텍스트를 인코딩해서 바이트 시퀀스로 만들어.
그다음 이 바이트 시퀀스에 BPE 알고리즘을 적용하는 거지.

자주 나오는 바이트 쌍을 찾아서 합치고, 이걸 반복해.
그러다 보면 우리가 원하는 토큰의 개수가 될 때까지 합칠 수 있어.

이렇게 해서 만든 게 바로 BPE 토크나이저야.
GPT-2나 GPT-4 토크나이저도 다 이 방식으로 만들어졌대.

물론 실제로는 더 복잡한 트릭들이 있긴 해.
가령 문자 범주를 나누는 규칙을 정한다던가, 특수 토큰을 추가한다던가 하는 것들 말야.

하여간 BPE만 잘 이해하면 너도 토크나이저 만드는 게 좀 더 쉬워질 거야.
BPE 식으로 문자를 합쳐 나가는 걸 여러 번 연습해 봐.
그럼 카파시처럼 LLM의 달인이 될 수 있을 걸?

### 3. 실전 GPT 토크나이저 분석
자, 이제 실전으로 가보자. GPT 토크나이저를 샅샅이 뜯어보는 거야.
GPT-2랑 GPT-4 토크나이저가 어떻게 다른지, 왜 다른지 알아보자고.

일단 GPT-2 토크나이저부터 볼까? 얘는 5만 개 좀 안 되는 토큰을 쓰거든.
그리고 토큰을 만들 때 정규 표현식을 써서 문자 범주를 나눠.
가령 문자, 숫자, 구두점 이런 걸 구분하는 거지.

예를 들어서 "Hello, world! 123"이라는 텍스트가 있다고 치자.
GPT-2 토크나이저는 이걸 ['Hello', ',', ' world', '!', ' 123']로 쪼갤 거야.
보이지? 문자, 구두점, 숫자가 따로따로 떨어지는 거.

이렇게 나눠 놓으면 'Hello'랑 '!'가 합쳐지는 일은 없겠지?
그래서 의도치 않은 토큰이 생기는 걸 막을 수 있어.

그런데 말야, GPT-2는 공백 문자를 제대로 처리를 못 해.
파이썬 코드 같은 걸 줘 봐. 스페이스 문자 때문에 토큰이 너무 많이 생기는 거야.
그래서 GPT-2로는 파이썬 코드 완성하기가 좀 힘들었어.

GPT-4는 이런 문제를 잘 해결했지. 토큰 수도 10만 개로 늘렸고.
공백 문자 처리도 훨씬 잘돼. 스페이스 4개까지는 하나의 토큰으로 처리한다니까.
그러니까 GPT-4는 파이썬 코드도 잘 완성하는 거야.

아, 그리고 GPT-2는 특수 토큰이 하나밖에 없어. 바로 'end of text'라는 토큰인데,
이건 문서의 끝을 표시하는 데 써. 그래서 문서와 문서 사이의 경계를 알려주지.

반면에 GPT-4는 특수 토큰이 훨씬 많아. 'end of text' 말고도 'Prim'이라는 토큰도 있어.
이건 'pll in the middle'의 줄임말인데, 텍스트 중간의 빈칸을 채우는 데 쓰는 거야.
뭐, 사실 우리가 알 필요는 없는 디테일이긴 하지만.

어쨌든 GPT-2에서 GPT-4로 오면서 tokenizer가 많이 발전한 건 사실이야.
토큰 수도 늘어났고, 공백 문자 처리도 개선되고, 특수 토큰도 다양해졌어.

이런 것까지 따져가면서 토크나이저를 만들어야 하는 거 알겠지?
그냥 대충 토큰 나누면 끝이 아니라니까.

### 4. Sentence Piece 라이브러리
자, 이번에는 Sentence Piece라는 라이브러리를 살펴보자.
이거 GPT 토크나이저랑은 좀 다른 방식으로 동작하거든.
그래서 둘이 어떻게 다른지 이해하는 게 중요해.

일단 Sentence Piece는 유니코드 코드포인트를 기준으로 BPE를 적용해.
그니까 바이트 단위가 아니라 코드포인트 단위로 쪼개는 거지.
그리고 나서 코드포인트를 합쳐 나가면서 토큰을 만드는 거야.

만약에 코드포인트가 너무 희귀해서 토큰에 포함되지 않으면 어떻게 될까?
그럴 때는 Byte Fallback이라는 걸 써. 그러니까 그 코드포인트를 UTF-8로 인코딩한 다음에,
바이트 단위로 쪼개서 특별한 바이트 토큰으로 만드는 거야.

그래서 Sentence Piece는 코드포인트 단위의 BPE와 바이트 단위의 Fallback을 같이 쓰는 거지.
이게 바로 Tick Token이랑 다른 점이야. Tick Token은 처음부터 끝까지 바이트 단위로 BPE를 적용하잖아.

근데 말야, Sentence Piece도 좀 quirk가 있어.
일단 UNK 토큰이 꼭 있어야 돼. 그리고 Byte Fallback할 때도 좀 이상한 면이 있고.
그리고 솔직히 문서화도 영 좋지 않아. 내가 직접 코드를 까봐야 작동 원리를 이해할 수 있더라고.

예를 들어서 korean이라는 단어가 있다고 치자.
korean은 토크나이저 학습 데이터에 없던 단어라서 코드포인트 단위로는 처리가 안 돼.
그럼 Byte Fallback을 써서 UTF-8 인코딩 결과인 [107, 111, 114, 101, 97, 110]을 토큰으로 만들어 버리는 거지.
좀 어색하지?
https://chatgpt.com/share/e4449337-5574-4a33-9528-1fd7edd2761b

그래도 장점도 있어. 일단 학습이랑 추론을 모두 지원하고, 속도도 빨라.
그리고 휴리스틱 말고도 여러가지 알고리즘을 쓸 수 있어서 융통성도 좋고.

언제 Sentence Piece를 쓰면 좋냐면, 일단 언어의 종류가 많은 경우야.
코드포인트 기반이다 보니까 다국어 텍스트 처리에 좀 더 유리하거든.
그리고 희귀 문자를 많이 다뤄야 하는 경우에도 Byte Fallback 덕분에 Sentence Piece가 낫지.

물론 quirk만 조심하면 돼. UNK 토큰 처리라던가 하는 건 꼭 신경 써야 해.
쓸데없는 설정은 다 꺼 놓고, 최대한 심플하게 쓰는 게 좋아.
Lama나 Mistral 같은 최신 LLM들은 다 Sentence Piece를 쓰더라고. 그런 걸 보면 잘 알아두면 좋겠지?

'''
quirk는 이상한 점, 별난 점, 그러니까 좀 특이한 부분을 말하는 거야.

Sentence Piece를 쓰다 보면 이상한 점이 좀 있어.
가령 UNK 토큰 말이야. UNK는 Unknown의 줄임말인데, 알 수 없는 토큰을 표시하는 거지.
Sentence Piece에서는 이 UNK 토큰이 꼭 있어야 돼. 없으면 에러가 나거든.

그리고 Byte Fallback도 좀 특이해.
UTF-8 인코딩 결과를 바이트 시퀀스로 만들어서 토큰화를 하는 건 좋은데,
이게 토크나이저 입장에서는 본래의 의미랑은 좀 멀어지는 거잖아.
korean을 코드포인트 단위로 파악하는 게 아니라 그냥 바이트 나열로 보는 거니까.

이런 것들이 바로 quirk야. 그냥 특이한 점, 이상한 점 정도로 이해하면 돼.
별로 중요한 건 아닌데, Sentence Piece를 실제로 써 보면 종종 마주치게 될 거야.
미리 알아두면 나중에 당황하지 않겠지?

아, 그리고 문서화가 부실한 것도 quirk라고 할 수 있겠다.
내가 Sentence Piece 쓸 때는 소스 코드를 직접 까봐야 했거든.
그래서 동작 원리를 이해하는 데 좀 오래 걸렸어. 이것도 내가 본 quirk 중 하나야.

요약하자면 quirk는 그냥 특이한 점, 이상한 점을 말하는 거야.
꼭 나쁜 건 아닌데, 그냥 조심해야 할 부분? 정도로 생각하면 될 것 같아.
Sentence Piece는 quirk가 좀 있지만 그래도 배워 둘 만한 라이브러리라고 생각해.
GPT나 Lama 같은 최신 LLM들도 다 Sentence Piece를 쓰고 있으니까 말이야.
'''

### 5. 토크나이저 성능 최적화 팁
자, 이번에는 토크나이저 성능을 최적화하는 노하우를 알려 줄게.
이거 알아두면 니 LLM 토크나이저도 효율적으로 만들 수 있을 거야.

일단 가장 중요한 건 Vocabulary 크기를 적절하게 설정하는 거야.
Vocab 크기가 너무 작으면 토큰이 부족해서 텍스트를 제대로 표현할 수 없어.
그렇다고 너무 크면 오히려 학습이 안 돼. 토큰 빈도수가 너무 낮아지니까.
보통 10만 개 내외로 설정하는 게 좋더라고. GPT-4도 그 정도 써.

그리고 희귀 토큰 처리도 중요해. 출현 빈도가 매우 낮은 토큰들 말이야.
이런 애들은 Vocab에서 아예 제외시켜 버리는 게 좋아. 대신 UNK 토큰으로 대체하는 거지.
아니면 Byte Fallback을 써서 바이트 단위로 쪼개는 것도 방법이야.
여하튼 희귀 토큰은 특별 취급해야 돼. 안 그러면 Vocab 크기만 늘어나고 학습은 안 돼.

토큰 density도 체크해 봐야 해. 이건 텍스트를 토큰으로 압축하는 비율을 말하는 거야.
density가 높으면 토큰 시퀀스가 짧아지니까 LLM이 더 많은 맥락을 볼 수 있어.
그런데 density가 지나치게 높으면 토큰 하나에 정보가 너무 많이 압축돼서 오히려 학습이 힘들어.
적정선을 찾는 게 중요하지. compression ratio로 한 1.5 정도 노려 보는 게 좋아.

다국어 데이터셋 다룰 때는 더 조심해야 돼.
영어랑 아시아권 언어는 토큰 특성이 많이 달라.
한국어나 일본어는 토큰이 더 잘게 쪼개지는 편이거든. 그러다 보니 시퀀스가 길어지고.
이럴 때는 좀 더 큰 Vocab 사이즈를 쓰는 게 낫겠지?
아니면 언어마다 토크나이저를 따로 만드는 것도 고려해 봐.

GPT 토크나이저 쓸 거면 TickToken 라이브러리 추천할게.
얘는 C++로 짜여 있어서 속도가 빨라. 10만 개 토큰을 처리하는 데 1초도 안 걸려.
Lama 스타일로 가고 싶으면 SentencePiece도 괜찮아.
얘는 Vocab 크기를 유연하게 조절할 수 있는 게 장점이지.

하여간 토크나이저 만들 때는 이런 것들 다 살펴봐야 해. 디테일이 성능을 좌우하니까.
안드레이 카프시도 이런 디테일에 엄청난 공을 들였대.
우리도 이런 tip만 잘 활용하면 고품질 토크나이저 만들 수 있을 거야. 화이팅!

### 6. 토크나이제이션 실수례와 교훈
자, 지금까지 토크나이저를 잘 만드는 법에 대해 배웠지?
그런데 현실은 녹록지 않아. 실수하기 딱 좋거든.
여기서는 실제로 있었던 실수 사례를 몇 가지 살펴보면서 교훈을 얻어 보자.

첫째는 불완전한 토큰으로 인한 모델 오작동이야.
가령 "depaul"이라는 단어가 있다고 치자. 근데 학습 데이터에는 "depaul t"만 있었어.
그러면 토크나이저는 "depaul"을 제대로 인식 못 하고 "de", "paul"로 쪼개 버리는 거야.
이렇게 되면 LLM은 "depaul"의 의미를 제대로 파악할 수 없게 돼.

해결책은 뭘까? 토큰 단위를 잘 조절하는 거야.
너무 짧게 쪼개면 이런 문제가 생기니까.
단어 단위로 토큰을 만들되, 접사나 어미는 분리하는 것도 좋은 방법이야.
Wordpiece라는 토크나이저가 그런 식으로 동작하거든.

두 번째 사례는 바로 Solid Gold Magikarp야. 이건 진짜 레전드급 사건이지.
"Solid Gold Magikarp"라는 표현이 모델을 엄청 혼란스럽게 만들더라고.
알고 보니 얘는 Reddit에서 활동하던 유저 이름이었어.
근데 토크나이저 학습 데이터에는 얘가 많이 나왔는데, 정작 LLM 학습 데이터에는 없었던 거야.
그래서 모델이 이 표현에 대해 엉뚱한 반응을 보인 거지.

이런 일을 막으려면 토크나이저랑 LLM을 같은 데이터로 학습시켜야 해.
토크나이저만 따로 학습시키면 이런 불일치가 생길 수 있거든.
그리고 데이터를 잘 필터링해서 노이즈를 최소화하는 것도 중요해.
Reddit 댓글 같은 걸 그대로 학습시키면 이런 오류가 생길 수밖에 없어.

마지막으로 데이터 포맷에 따른 토큰 효율성 문제도 있어.
JSON이랑 YAML로 같은 내용을 표현하면 토큰 개수가 다르더라고. YAML이 더 효율적이야.
왜냐하면 JSON은 중괄호, 콜론, 쉼표 같은 걸 많이 써서 토큰이 늘어나거든.

이건 어떻게 해결하냐? 가능하면 YAML 같이 간결한 포맷을 쓰는 게 좋아.
아니면 압축률이 높은 바이너리 포맷을 쓰는 것도 방법이야.
Protocol Buffer나 MessagePack 같은 걸 활용하면 돼.

이런 실수례를 보면 토크나이저가 얼마나 까다로운지 알 수 있어.
디테일을 놓치면 성능이 확 떨어지는 거야.
그래서 데이터 전처리부터 토큰 생성 규칙, 포맷 선택까지 꼼꼼히 챙겨야 돼.
실수를 최소화하는 게 고품질 토크나이저의 비결이라고 할 수 있지.

### 7. 카파시의 조언과 전망
자, 이제 대망의 마지막 시간이야. 여기서는 안드레이 카프시가 직접 토크나이제이션에 대해 어떻게 생각하는지 들어 보자.
사실 이 강의 자료를 만들면서 카프시한테 직접 연락해서 조언을 구했거든.

일단 카프시는 최근 토크나이제이션 연구 동향에 대해 이렇게 말했어.
"요즘은 BPE 말고도 Unigram이나 Wordpiece 같은 새로운 알고리즘들이 많이 나오고 있어.
특히 Unigram은 BPE보다 더 효율적인 토큰을 만들 수 있다는 점에서 주목받고 있지."
Unigram이 뭐냐면 토큰의 출현 확률을 고려해서 토큰을 만드는 알고리즘이야.
BPE처럼 빈도수만 보는 게 아니라 좀 더 통계적인 접근을 하는 거지.

그리고 카프시는 이상적인 토크나이저의 조건에 대해서도 언급했어.
"좋은 토크나이저라면 언어에 독립적이어야 해.
영어든 한국어든 일본어든 모든 언어의 텍스트를 잘 토큰화할 수 있어야 하는 거지.
그리고 희귀 단어나 신조어도 잘 처리할 수 있어야 돼.
이걸 위해서는 형태소 분석이나 음절 토크나이제이션 같은 기술이 필요할 거야."

또 하나, 카프시는 토크나이제이션의 한계에 대해서도 지적했어.
"아무리 토크나이제이션을 잘한다고 해도 한계는 있어.
결국 텍스트를 완벽하게 이해하려면 의미론적인 분석이 필요한데, 그건 토큰만 가지고는 어려워.
의미 정보를 반영할 수 있는 임베딩을 토큰에 추가하는 방법을 고민해 봐야 할 거야."

마지막으로 카프시는 이런 말을 남겼어.
"토크나이제이션은 참 재미있는 분야야. 간단해 보이지만 깊이 파고들면 끝이 없지.
지금까지 많은 발전이 있었지만 아직도 개선의 여지가 많아.
특히 다국어 토크나이제이션이나 Byte-to-Byte 토크나이제이션 같은 건 앞으로 더 연구해 볼 만한 주제라고 생각해."

어때, 카프시의 조언 듣고 나니까 토크나이제이션이 좀 더 잡히지 않아?
사실 난 이 강의 준비하면서 카프시한테 많이 배웠어.
토크나이제이션의 중요성도 깨달았고, 앞으로 어떻게 발전시켜 나가야 할지도 감이 좀 잡혔지.

내 생각에 토크나이제이션의 미래는 Unigram이나 Byte-to-Byte 같은 새로운 방법에 있어.
기존의 BPE나 WordPiece를 뛰어넘는 혁신적인 알고리즘이 나올 거라고 봐.
그리고 의미 정보를 반영하는 토큰 임베딩 같은 것도 적극 활용되겠지.

물론 그 과정이 순탄치만은 않을 거야.
다국어 토크나이제이션처럼 극복해야 할 과제도 많고.
하지만 우리가 계속 도전하다 보면 언젠가는 완벽에 가까운 토크나이저를 만들 수 있을 거야.

난 카프시처럼 토크나이제이션의 무한한 가능성을 믿어 의심치 않아.

자, 이제 이 강의의 마지막 코너를 소개할 시간이야.
바로 실습이지. 이론만 배웠다고 끝이 아니야.
직접 토크나이저를 만들어 봐야 제대로 이해할 수 있는 거 아냐?
그래서 마지막으로 MinBPE로 실습하는 시간을 가질 거야. 코드도 줄 테니까 너무 걱정 마.
이 실습만 잘 따라오면 너도 이제 토크나이제이션 마스터야!


### 부록. MinBPE 토크나이저 실습
자, 드디어 실습 시간이야! 토크나이제이션 이론만 줄창 듣다가 지루했지?
이제 직접 토크나이저를 만들어 볼 거니까 좀 신난다, 그치?

실습은 MinBPE라는 내가 만든 토크나이저로 할 거야.
MinBPE는 BPE 알고리즘을 간단명료하게 구현한 라이브러리야.
파이썬으로 짜여 있어서 누구나 쉽게 사용할 수 있지.

MinBPE의 특징은 심플함이야.
토크나이저에 정말 필요한 기능만 담고 있거든.
그래서 내부 동작을 이해하기가 쉬워.
딱 우리가 배운 BPE 알고리즘이 그대로 구현되어 있다고 보면 돼.

MinBPE의 또 다른 장점은 디버깅이 편하다는 거야.
각 단계별로 토큰화 결과를 출력해 주거든.
그래서 내가 의도한 대로 토큰이 만들어지는지 확인하기가 좋아.
디버깅할 때 요긴하게 쓸 수 있을 거야.

자 그럼 실습 들어가 볼까?
일단 MinBPE 깃헙에서 라이브러리를 다운받아.

pip install minbpe

이렇게 터미널에 치면 금방 설치할 수 있어.

다음으로 실습용 데이터를 준비하자.
여기서는 영어 위키피디아에서 발췌한 텍스트를 쓸 거야.
MinBPE 깃헙의 data 폴더에 있으니까 그걸 다운받아.

이제 실습 코드를 봐 보자. 주피터 노트북 파일로 준비해 뒀어.
앞부분은 라이브러리 import하고 데이터 로드하는 과정이니까 그냥 넘어가.

여기서부터가 토크나이저를 학습시키는 부분이야.
MinBPE는 여러 인자를 받는데, 중요한 건 vocab_size야.
우리는 1000으로 설정할 거야. 충분히 작으면서도 의미 있는 토큰을 만들 수 있거든.

mbpe = minbpe.MinBPE(vocab_size=1000)
mbpe.fit(texts)  # trains MinBPE on the input texts

fit 함수에 우리가 준비한 텍스트를 넣어 주면 토크나이저 학습이 끝나.
참 쉽지? SentencePiece처럼 수십 개 옵션 설정할 필요가 없어서 좋아.

자 이제 학습된 토크나이저로 텍스트를 토큰화해 보자.

tokens = mbpe.encode("This is an example sentence.")
print(tokens)

출력 결과: [32, 162, 23, 495, 1629, 4, 38, 1659, 4, 279, 4, 602, 5, 60, 284, 4, 843, 5, 195, 13, 191]
encode 함수에 문장을 넣어 주기만 하면 돼.
한 번에 여러 문장도 토큰화할 수 있어.
리스트로 문장을 넣어 주면 돼.

decoded_text = mbpe.decode(tokens)
print(decoded_text)

출력 결과: This is an example sentence.
디코딩도 마찬가지야. decode 함수에 토큰 리스트를 넣어 주면
원래 문장으로 돌려 줘.

자 이제 우리도 직접 해 보자.
토크나이저를 학습시킬 텍스트를 준비해.
토크나이저를 학습시켜 보고, 임의의 문장을 토큰화도 해 봐.
그리고 vocab_size를 바꿔 가면서 실험도 해 보자.
토큰 길이가 어떻게 달라지는지 관찰해 봐.

이상으로 MinBPE 실습을 마칠 거야.
어때, 직접 해 보니까 감이 좀 오지?
이 실습 예제로 연습 많이 해 봐.
BPE 알고리즘이 손에 익을 때까지 반복하는 거야.

MinBPE 실습이 끝났으니 이제 강의도 마무리할 시간이야.
토크나이제이션에 대해 아주 깊이 있게 살펴봤는데 재미있었어.
혹시 이해 안 되는 부분 있으면 나한테 DM 해
기본만 확실히 다져 놓으면 넌 앞으로 토크나이제이션 달인이 될 거야.

자 이제 가서 직접 토크나이저 만들어 봐.
넌 할 수 있어. 안드레이 카프시처럼!
토크나이제이션 마스터로 가는 너의 여정을 응원할게.
고생 많았어, 수고!
